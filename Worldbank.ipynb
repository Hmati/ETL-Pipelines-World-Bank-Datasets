{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipelines  | World Bank Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Extract, Transform, Load.**\n",
    "\n",
    "Data Sources :\n",
    "\n",
    "[World Bank Indicator Data](https://data.worldbank.org/indicator) - This data contains socio-economic indicators for countries around the world. A few example indicators include population, arable land, and central government debt.\n",
    "\n",
    "[World Bank Project Data](https://datacatalog.worldbank.org/dataset/world-bank-projects-operations) - This data set contains information about World Bank project lending since 1947.\n",
    "\n",
    "#### Outline of this notebook:\n",
    "**Extract data from different sources such as:**\n",
    "\n",
    "- csv files\n",
    "- json files\n",
    "- APIs\n",
    "\n",
    "**Transform data**\n",
    "- combining data from different sources\n",
    "- data cleaning\n",
    "- data types\n",
    "- parsing dates\n",
    "- file encodings\n",
    "- missing data\n",
    "- duplicate data\n",
    "- dummy variables\n",
    "- remove outliers\n",
    "- scaling features\n",
    "- engineering features\n",
    "\n",
    "**Load**\n",
    "- send the transformed data to a database\n",
    "\n",
    "**ETL Pipeline**\n",
    "- code an ETL pipeline\n",
    "\n",
    "The goal of this project is to clean and merge World Bank datasets into a single, unified table, ready for use in a machine learning model that predicts the total costs of World Bank projects. This involves designing an ETL pipeline that extracts data from various sources, applies necessary transformations, and loads the processed data into a new database. By the end, the project consolidates these datasets through a streamlined Python module that handles extraction, transformation, and loading in one seamless process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries for the project\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  ##  Extracting data from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\electricity_access_percent.csv\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\etl-pipelines-tutorial-world-bank-datasets.ipynb\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\Projects.csv\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\README.md\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\Worldbank.ipynb\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\COMMIT_EDITMSG\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\config\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\description\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\HEAD\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\index\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\packed-refs\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\applypatch-msg.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\commit-msg.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\fsmonitor-watchman.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\post-update.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\pre-applypatch.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\pre-commit.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\pre-merge-commit.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\pre-push.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\pre-rebase.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\pre-receive.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\prepare-commit-msg.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\push-to-checkout.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\sendemail-validate.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\hooks\\update.sample\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\info\\exclude\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\logs\\HEAD\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\logs\\refs\\heads\\master\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\logs\\refs\\remotes\\origin\\HEAD\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\logs\\refs\\remotes\\origin\\master\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\28\\aa5a816acdbe7617f8f072b8ab6961300a5d5f\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\36\\3fcab7ed6e9634e198cf5555ceb88932c9a245\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\86\\3f97c577e84e5bf13fde9db4b4f57e1093c2a4\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\91\\51543255287ec41bf1696be98147d7d02e11f7\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\b9\\2c50336367504caadb5f3382b732635958f296\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\ca\\6bd06b8f5b1ad9a3ab52dc279efbd8c8781257\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\pack\\pack-da68e7fa15307268d897854b11720f7a419f2934.idx\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\pack\\pack-da68e7fa15307268d897854b11720f7a419f2934.pack\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\objects\\pack\\pack-da68e7fa15307268d897854b11720f7a419f2934.rev\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\refs\\heads\\master\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\refs\\remotes\\origin\\HEAD\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.git\\refs\\remotes\\origin\\master\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.ipynb_checkpoints\\etl-pipelines-tutorial-world-bank-datasets-checkpoint.ipynb\n",
      "C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets\\.ipynb_checkpoints\\Worldbank-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = r'C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Pipelines-World-Bank-Datasets'\n",
    "\n",
    "for dirname, _, filenames in os.walk(directory_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the projects_data.csv file using the pandas library\n",
    "\n",
    "df_projects = pd.read_csv('Projects.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a dtype warning, Read about what this warning is in the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.errors.DtypeWarning.html).\n",
    "\n",
    "Pandas tries to figure out programatically the data type of each column (integer, float, boolean, string). In this case, pandas could not automatically figure out the data type. That is because some columns have more than one possible data types. In other words, this data is messy.\n",
    "\n",
    "You can use the dtype option to specify the data type of each column. Because there are so many columns in this data set, you can set all columns to be strings at least for now.\n",
    "\n",
    "Try reading in the data set again using the read_csv() method. This time, also use the option dtype=str so that pandas treats everything like a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the projects_data.csv file using the read_csv method and dtype = str option\n",
    "df_projects =  pd.read_csv('Projects.csv',dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Country</th>\n",
       "      <th>Project Status</th>\n",
       "      <th>Last Stage Reached Name</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Project Development Objective</th>\n",
       "      <th>Implementing Agency</th>\n",
       "      <th>Board Approval Date</th>\n",
       "      <th>Project Closing Date</th>\n",
       "      <th>...</th>\n",
       "      <th>IDA Commitment</th>\n",
       "      <th>Grant Amount</th>\n",
       "      <th>Total IBRD, IDA and Grant Commitment</th>\n",
       "      <th>Borrower</th>\n",
       "      <th>Lending Instrument</th>\n",
       "      <th>Environmental Assessment Category</th>\n",
       "      <th>Environmental and Social Risk</th>\n",
       "      <th>Associated Project</th>\n",
       "      <th>Consultant Services Required</th>\n",
       "      <th>Financing Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>regionname</td>\n",
       "      <td>countryshortname</td>\n",
       "      <td>status</td>\n",
       "      <td>last_stage_reached_name</td>\n",
       "      <td>project_name</td>\n",
       "      <td>pdo</td>\n",
       "      <td>impagency</td>\n",
       "      <td>boardapprovaldate</td>\n",
       "      <td>closingdate</td>\n",
       "      <td>...</td>\n",
       "      <td>idacommamt</td>\n",
       "      <td>grantamt</td>\n",
       "      <td>curr_total_commitment</td>\n",
       "      <td>borrower</td>\n",
       "      <td>lendinginstr</td>\n",
       "      <td>envassesmentcategorycode</td>\n",
       "      <td>esrc_ovrl_risk_rate</td>\n",
       "      <td>supplementprojectflg</td>\n",
       "      <td>cons_serv_reqd_ind</td>\n",
       "      <td>projectfinancialtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P000001</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Closed</td>\n",
       "      <td>BANK APPROVED</td>\n",
       "      <td>West Africa Pilot Community-based Natural Reso...</td>\n",
       "      <td>CONSERVATION OF BIO-DIVERSITY.  OBJECTIVE IS T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995-09-14T00:00:00Z</td>\n",
       "      <td>6/30/2004</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11400000</td>\n",
       "      <td>11400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Investment Loan</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P000002</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Dropped</td>\n",
       "      <td>BANK APPROVED</td>\n",
       "      <td>LAKE VICTORIA ENVIRO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997-06-30T00:00:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P000003</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Closed</td>\n",
       "      <td>BANK APPROVED</td>\n",
       "      <td>REIMP(CEN.ENV.INFO)</td>\n",
       "      <td>REIMP TO IMPROVE/STRENGTHEN PLANNING &amp; MNGT. O...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997-12-18T00:00:00Z</td>\n",
       "      <td>6/30/2003</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16700000</td>\n",
       "      <td>16700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Investment Loan</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P000004</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Dropped</td>\n",
       "      <td>BANK APPROVED</td>\n",
       "      <td>P.TA/SADCC TRADE DEV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991-06-30T00:00:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Investment Loan</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Project ID      Region           Country Project Status  \\\n",
       "0         id  regionname  countryshortname         status   \n",
       "1    P000001      Africa            Africa         Closed   \n",
       "2    P000002      Africa            Africa        Dropped   \n",
       "3    P000003      Africa            Africa         Closed   \n",
       "4    P000004      Africa            Africa        Dropped   \n",
       "\n",
       "   Last Stage Reached Name                                       Project Name  \\\n",
       "0  last_stage_reached_name                                       project_name   \n",
       "1            BANK APPROVED  West Africa Pilot Community-based Natural Reso...   \n",
       "2            BANK APPROVED                               LAKE VICTORIA ENVIRO   \n",
       "3            BANK APPROVED                                REIMP(CEN.ENV.INFO)   \n",
       "4            BANK APPROVED                               P.TA/SADCC TRADE DEV   \n",
       "\n",
       "                      Project Development Objective  Implementing Agency  \\\n",
       "0                                                pdo           impagency   \n",
       "1  CONSERVATION OF BIO-DIVERSITY.  OBJECTIVE IS T...                 NaN   \n",
       "2                                                NaN                 NaN   \n",
       "3  REIMP TO IMPROVE/STRENGTHEN PLANNING & MNGT. O...                 NaN   \n",
       "4                                                NaN                 NaN   \n",
       "\n",
       "    Board Approval Date Project Closing Date  ... IDA Commitment Grant Amount  \\\n",
       "0     boardapprovaldate          closingdate  ...     idacommamt     grantamt   \n",
       "1  1995-09-14T00:00:00Z            6/30/2004  ...            NaN     11400000   \n",
       "2  1997-06-30T00:00:00Z                  NaN  ...            NaN          NaN   \n",
       "3  1997-12-18T00:00:00Z            6/30/2003  ...            NaN     16700000   \n",
       "4  1991-06-30T00:00:00Z                  NaN  ...            NaN          NaN   \n",
       "\n",
       "  Total IBRD, IDA and Grant Commitment  Borrower        Lending Instrument  \\\n",
       "0                curr_total_commitment  borrower              lendinginstr   \n",
       "1                             11400000       NaN  Specific Investment Loan   \n",
       "2                                  NaN       NaN                       NaN   \n",
       "3                             16700000       NaN  Specific Investment Loan   \n",
       "4                                  NaN       NaN  Specific Investment Loan   \n",
       "\n",
       "  Environmental Assessment Category Environmental and Social Risk  \\\n",
       "0          envassesmentcategorycode           esrc_ovrl_risk_rate   \n",
       "1                                 B                           NaN   \n",
       "2                                 B                           NaN   \n",
       "3                                 C                           NaN   \n",
       "4                                 C                           NaN   \n",
       "\n",
       "     Associated Project Consultant Services Required        Financing Type  \n",
       "0  supplementprojectflg           cons_serv_reqd_ind  projectfinancialtype  \n",
       "1                     N                          NaN                 Other  \n",
       "2                   NaN                          NaN                   NaN  \n",
       "3                     N                          NaN                 Other  \n",
       "4                   NaN                          NaN                   NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the cell below to see what the data looks like\n",
    "df_projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project ID                                  0\n",
       "Region                                      0\n",
       "Country                                     0\n",
       "Project Status                              0\n",
       "Last Stage Reached Name                   796\n",
       "Project Name                                0\n",
       "Project Development Objective           11776\n",
       "Implementing Agency                     19455\n",
       "Board Approval Date                        79\n",
       "Project Closing Date                    14825\n",
       "Current Project Cost                      346\n",
       "IBRD Commitment                         14210\n",
       "IDA Commitment                          14215\n",
       "Grant Amount                            10029\n",
       "Total IBRD, IDA and Grant Commitment    10029\n",
       "Borrower                                19707\n",
       "Lending Instrument                        836\n",
       "Environmental Assessment Category       11481\n",
       "Environmental and Social Risk           25186\n",
       "Associated Project                      10115\n",
       "Consultant Services Required            21957\n",
       "Financing Type                          10044\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of null values in the data set\n",
    "df_projects.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are notable number of missing values in the table \n",
    "\n",
    "Next, output the shape of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27062, 22)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  output the shape of the data frame\n",
    "df_projects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the pandas read_csv method to read in the population_data.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1961</th>\n",
       "      <th>1962</th>\n",
       "      <th>1963</th>\n",
       "      <th>1964</th>\n",
       "      <th>1965</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.401410</td>\n",
       "      <td>17.911234</td>\n",
       "      <td>18.463874</td>\n",
       "      <td>18.924037</td>\n",
       "      <td>19.437054</td>\n",
       "      <td>20.026254</td>\n",
       "      <td>20.647969</td>\n",
       "      <td>21.165877</td>\n",
       "      <td>21.863139</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.RU.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.728819</td>\n",
       "      <td>7.005877</td>\n",
       "      <td>7.308571</td>\n",
       "      <td>7.547226</td>\n",
       "      <td>7.875917</td>\n",
       "      <td>8.243018</td>\n",
       "      <td>8.545483</td>\n",
       "      <td>8.906711</td>\n",
       "      <td>9.261320</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.UR.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.080931</td>\n",
       "      <td>38.422282</td>\n",
       "      <td>38.722108</td>\n",
       "      <td>38.993157</td>\n",
       "      <td>39.337872</td>\n",
       "      <td>39.695279</td>\n",
       "      <td>40.137847</td>\n",
       "      <td>40.522209</td>\n",
       "      <td>41.011132</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>EG.ELC.ACCS.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.860474</td>\n",
       "      <td>33.903800</td>\n",
       "      <td>38.854624</td>\n",
       "      <td>40.199898</td>\n",
       "      <td>43.017148</td>\n",
       "      <td>44.381259</td>\n",
       "      <td>46.264875</td>\n",
       "      <td>48.100862</td>\n",
       "      <td>48.711995</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to electricity, rural (% of rural popul...</td>\n",
       "      <td>EG.ELC.ACCS.RU.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.619475</td>\n",
       "      <td>16.500171</td>\n",
       "      <td>24.605861</td>\n",
       "      <td>25.396929</td>\n",
       "      <td>27.037528</td>\n",
       "      <td>29.137914</td>\n",
       "      <td>31.001049</td>\n",
       "      <td>32.777910</td>\n",
       "      <td>33.747907</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Country Name Country Code  \\\n",
       "0  Africa Eastern and Southern          AFE   \n",
       "1  Africa Eastern and Southern          AFE   \n",
       "2  Africa Eastern and Southern          AFE   \n",
       "3  Africa Eastern and Southern          AFE   \n",
       "4  Africa Eastern and Southern          AFE   \n",
       "\n",
       "                                      Indicator Name     Indicator Code  1960  \\\n",
       "0  Access to clean fuels and technologies for coo...     EG.CFT.ACCS.ZS   NaN   \n",
       "1  Access to clean fuels and technologies for coo...  EG.CFT.ACCS.RU.ZS   NaN   \n",
       "2  Access to clean fuels and technologies for coo...  EG.CFT.ACCS.UR.ZS   NaN   \n",
       "3            Access to electricity (% of population)     EG.ELC.ACCS.ZS   NaN   \n",
       "4  Access to electricity, rural (% of rural popul...  EG.ELC.ACCS.RU.ZS   NaN   \n",
       "\n",
       "   1961  1962  1963  1964  1965  ...       2014       2015       2016  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN  ...  17.401410  17.911234  18.463874   \n",
       "1   NaN   NaN   NaN   NaN   NaN  ...   6.728819   7.005877   7.308571   \n",
       "2   NaN   NaN   NaN   NaN   NaN  ...  38.080931  38.422282  38.722108   \n",
       "3   NaN   NaN   NaN   NaN   NaN  ...  31.860474  33.903800  38.854624   \n",
       "4   NaN   NaN   NaN   NaN   NaN  ...  17.619475  16.500171  24.605861   \n",
       "\n",
       "        2017       2018       2019       2020       2021       2022  2023  \n",
       "0  18.924037  19.437054  20.026254  20.647969  21.165877  21.863139   NaN  \n",
       "1   7.547226   7.875917   8.243018   8.545483   8.906711   9.261320   NaN  \n",
       "2  38.993157  39.337872  39.695279  40.137847  40.522209  41.011132   NaN  \n",
       "3  40.199898  43.017148  44.381259  46.264875  48.100862  48.711995   NaN  \n",
       "4  25.396929  27.037528  29.137914  31.001049  32.777910  33.747907   NaN  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the population_data.csv file using the read_csv() method\n",
    "# Put the results in a variable called df_population\n",
    "df_population = pd.read_csv('population.csv')\n",
    "df_population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country Name           0\n",
       "Country Code           0\n",
       "Indicator Name         0\n",
       "Indicator Code         0\n",
       "1960              360296\n",
       "                   ...  \n",
       "2019              171440\n",
       "2020              189237\n",
       "2021              203227\n",
       "2022              234943\n",
       "2023              315794\n",
       "Length: 68, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of null values in each column\n",
    "df_population.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like every year column has at least one NaN value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         41\n",
       "1         41\n",
       "2         41\n",
       "3         41\n",
       "4         43\n",
       "          ..\n",
       "397931    60\n",
       "397932    57\n",
       "397933    57\n",
       "397934    31\n",
       "397935    31\n",
       "Length: 397936, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the null values by column\n",
    "df_population.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks like almost every row has only one null value. T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1961</th>\n",
       "      <th>1962</th>\n",
       "      <th>1963</th>\n",
       "      <th>1964</th>\n",
       "      <th>1965</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.401410</td>\n",
       "      <td>17.911234</td>\n",
       "      <td>18.463874</td>\n",
       "      <td>18.924037</td>\n",
       "      <td>19.437054</td>\n",
       "      <td>20.026254</td>\n",
       "      <td>20.647969</td>\n",
       "      <td>21.165877</td>\n",
       "      <td>21.863139</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.RU.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.728819</td>\n",
       "      <td>7.005877</td>\n",
       "      <td>7.308571</td>\n",
       "      <td>7.547226</td>\n",
       "      <td>7.875917</td>\n",
       "      <td>8.243018</td>\n",
       "      <td>8.545483</td>\n",
       "      <td>8.906711</td>\n",
       "      <td>9.261320</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.UR.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.080931</td>\n",
       "      <td>38.422282</td>\n",
       "      <td>38.722108</td>\n",
       "      <td>38.993157</td>\n",
       "      <td>39.337872</td>\n",
       "      <td>39.695279</td>\n",
       "      <td>40.137847</td>\n",
       "      <td>40.522209</td>\n",
       "      <td>41.011132</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>EG.ELC.ACCS.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.860474</td>\n",
       "      <td>33.903800</td>\n",
       "      <td>38.854624</td>\n",
       "      <td>40.199898</td>\n",
       "      <td>43.017148</td>\n",
       "      <td>44.381259</td>\n",
       "      <td>46.264875</td>\n",
       "      <td>48.100862</td>\n",
       "      <td>48.711995</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to electricity, rural (% of rural popul...</td>\n",
       "      <td>EG.ELC.ACCS.RU.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.619475</td>\n",
       "      <td>16.500171</td>\n",
       "      <td>24.605861</td>\n",
       "      <td>25.396929</td>\n",
       "      <td>27.037528</td>\n",
       "      <td>29.137914</td>\n",
       "      <td>31.001049</td>\n",
       "      <td>32.777910</td>\n",
       "      <td>33.747907</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397931</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Women who believe a husband is justified in be...</td>\n",
       "      <td>SG.VAW.REFU.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397932</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Women who were first married by age 15 (% of w...</td>\n",
       "      <td>SP.M15.2024.FE.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397933</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Women who were first married by age 18 (% of w...</td>\n",
       "      <td>SP.M18.2024.FE.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397934</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Women's share of population ages 15+ living wi...</td>\n",
       "      <td>SH.DYN.AIDS.FE.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>59.606951</td>\n",
       "      <td>59.740456</td>\n",
       "      <td>59.888983</td>\n",
       "      <td>60.053623</td>\n",
       "      <td>60.216147</td>\n",
       "      <td>60.377610</td>\n",
       "      <td>60.551609</td>\n",
       "      <td>60.693180</td>\n",
       "      <td>60.825294</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397935</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Young people (ages 15-24) newly infected with HIV</td>\n",
       "      <td>SH.HIV.INCD.YG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18000.000000</td>\n",
       "      <td>16000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>11000.000000</td>\n",
       "      <td>8400.000000</td>\n",
       "      <td>6900.000000</td>\n",
       "      <td>5900.000000</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>4900.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>373719 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Country Name Country Code  \\\n",
       "0       Africa Eastern and Southern          AFE   \n",
       "1       Africa Eastern and Southern          AFE   \n",
       "2       Africa Eastern and Southern          AFE   \n",
       "3       Africa Eastern and Southern          AFE   \n",
       "4       Africa Eastern and Southern          AFE   \n",
       "...                             ...          ...   \n",
       "397931                     Zimbabwe          ZWE   \n",
       "397932                     Zimbabwe          ZWE   \n",
       "397933                     Zimbabwe          ZWE   \n",
       "397934                     Zimbabwe          ZWE   \n",
       "397935                     Zimbabwe          ZWE   \n",
       "\n",
       "                                           Indicator Name     Indicator Code  \\\n",
       "0       Access to clean fuels and technologies for coo...     EG.CFT.ACCS.ZS   \n",
       "1       Access to clean fuels and technologies for coo...  EG.CFT.ACCS.RU.ZS   \n",
       "2       Access to clean fuels and technologies for coo...  EG.CFT.ACCS.UR.ZS   \n",
       "3                 Access to electricity (% of population)     EG.ELC.ACCS.ZS   \n",
       "4       Access to electricity, rural (% of rural popul...  EG.ELC.ACCS.RU.ZS   \n",
       "...                                                   ...                ...   \n",
       "397931  Women who believe a husband is justified in be...     SG.VAW.REFU.ZS   \n",
       "397932  Women who were first married by age 15 (% of w...  SP.M15.2024.FE.ZS   \n",
       "397933  Women who were first married by age 18 (% of w...  SP.M18.2024.FE.ZS   \n",
       "397934  Women's share of population ages 15+ living wi...  SH.DYN.AIDS.FE.ZS   \n",
       "397935  Young people (ages 15-24) newly infected with HIV     SH.HIV.INCD.YG   \n",
       "\n",
       "        1960  1961  1962  1963  1964  1965  ...          2014          2015  \\\n",
       "0        NaN   NaN   NaN   NaN   NaN   NaN  ...     17.401410     17.911234   \n",
       "1        NaN   NaN   NaN   NaN   NaN   NaN  ...      6.728819      7.005877   \n",
       "2        NaN   NaN   NaN   NaN   NaN   NaN  ...     38.080931     38.422282   \n",
       "3        NaN   NaN   NaN   NaN   NaN   NaN  ...     31.860474     33.903800   \n",
       "4        NaN   NaN   NaN   NaN   NaN   NaN  ...     17.619475     16.500171   \n",
       "...      ...   ...   ...   ...   ...   ...  ...           ...           ...   \n",
       "397931   NaN   NaN   NaN   NaN   NaN   NaN  ...           NaN     14.500000   \n",
       "397932   NaN   NaN   NaN   NaN   NaN   NaN  ...           NaN      3.700000   \n",
       "397933   NaN   NaN   NaN   NaN   NaN   NaN  ...           NaN     32.400000   \n",
       "397934   NaN   NaN   NaN   NaN   NaN   NaN  ...     59.606951     59.740456   \n",
       "397935   NaN   NaN   NaN   NaN   NaN   NaN  ...  18000.000000  16000.000000   \n",
       "\n",
       "                2016          2017         2018         2019         2020  \\\n",
       "0          18.463874     18.924037    19.437054    20.026254    20.647969   \n",
       "1           7.308571      7.547226     7.875917     8.243018     8.545483   \n",
       "2          38.722108     38.993157    39.337872    39.695279    40.137847   \n",
       "3          38.854624     40.199898    43.017148    44.381259    46.264875   \n",
       "4          24.605861     25.396929    27.037528    29.137914    31.001049   \n",
       "...              ...           ...          ...          ...          ...   \n",
       "397931           NaN           NaN          NaN          NaN          NaN   \n",
       "397932           NaN           NaN          NaN     5.400000          NaN   \n",
       "397933           NaN           NaN          NaN    33.700000          NaN   \n",
       "397934     59.888983     60.053623    60.216147    60.377610    60.551609   \n",
       "397935  14000.000000  11000.000000  8400.000000  6900.000000  5900.000000   \n",
       "\n",
       "               2021         2022  2023  \n",
       "0         21.165877    21.863139   NaN  \n",
       "1          8.906711     9.261320   NaN  \n",
       "2         40.522209    41.011132   NaN  \n",
       "3         48.100862    48.711995   NaN  \n",
       "4         32.777910    33.747907   NaN  \n",
       "...             ...          ...   ...  \n",
       "397931          NaN          NaN   NaN  \n",
       "397932          NaN          NaN   NaN  \n",
       "397933          NaN          NaN   NaN  \n",
       "397934    60.693180    60.825294   NaN  \n",
       "397935  5200.000000  4900.000000   NaN  \n",
       "\n",
       "[373719 rows x 68 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code outputs any row that contains a null value\n",
    "# The purpose is to see what rows contain null values now that \n",
    "\n",
    "df_population[df_population.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Extract from JSON and XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ll be extracting population data once more, but this time from a different file format.\n",
    "\n",
    "Both JSON and XML are commonly used for storing and transferring data. XML has been around longer, but JSON has gained popularity, especially for data exchange in web APIs (which I’ll dive into later). Many sources, like the World Bank, offer the same data in both JSON and XML formats.\n",
    "\n",
    "Let’s start by working with the JSON version of the population data to get familiar with extracting data in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lines(n, file_name):\n",
    "    f = open(file_name)\n",
    "    for i in range(n):\n",
    "        print(f.readline())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\n",
      "    \"{urn:oasis:names:tc:opendocument:xmlns:office:1.0}meta\": {\n",
      "\n",
      "        \"{urn:oasis:names:tc:opendocument:xmlns:meta:1.0}document-statistic\": null,\n",
      "\n",
      "        \"{urn:oasis:names:tc:opendocument:xmlns:meta:1.0}generator\": \"LibreOffice/24.8.1.2$Linux_X86_64 LibreOffice_project/87fa9aec1a63e70835390b81c40bb8993f1d4ff6\"\n",
      "\n",
      "    },\n",
      "\n",
      "    \"{urn:oasis:names:tc:opendocument:xmlns:office:1.0}settings\": {\n",
      "\n",
      "        \"{urn:oasis:names:tc:opendocument:xmlns:config:1.0}config-item-set\": [\n",
      "\n",
      "            {\n",
      "\n",
      "                \"{urn:oasis:names:tc:opendocument:xmlns:config:1.0}config-item\": [\n",
      "\n",
      "                    \"0\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_lines(10, 'output.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This JSON data appears to contain metadata and configuration settings for a document. Here’s a simplified breakdown of each key part:\n",
    "\n",
    "1. **`meta`**: This section holds general information about the document. \n",
    "   - **`document-statistic`** is `null`, which suggests there’s no specific statistic data provided for the document.\n",
    "   - **`generator`** identifies the software used to create or edit the document, which in this case is **LibreOffice** on Linux.\n",
    "\n",
    "2. **`settings`**: This section stores settings related to document configuration.\n",
    "   - **`config-item-set`** is an array, meaning it can contain multiple configuration sets.\n",
    "     - Each set has **`config-item`** entries, which appear to be settings or options. Here, `config-item` has the value `\"0\"`, which might be a default or placeholder setting for this particular document.\n",
    "\n",
    "In short, this JSON provides metadata about the document's creation tool and some basic configuration options. It’s formatted using URNs (Uniform Resource Names) to ensure compatibility with OpenDocument standards.\n",
    "\n",
    "Next, read in the population_data.json file using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}meta</th>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}settings</th>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}scripts</th>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}font-face-decls</th>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}styles</th>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}automatic-styles</th>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}master-styles</th>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:meta:1.0}document-statistic</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:meta:1.0}generator</th>\n",
       "      <td>LibreOffice/24.8.1.2$Linux_X86_64 LibreOffice_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:config:1.0}config-item-set</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'{urn:oasis:names:tc:opendocument:xmlns:conf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:office:1.0}script</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'{http://openoffice.org/2004/office}libraries...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{urn:oasis:names:tc:opendocument:xmlns:style:1.0}font-face</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None, None, None, None]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   {urn:oasis:names:tc:opendocument:xmlns:office:1.0}meta  \\\n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                               None       \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...  LibreOffice/24.8.1.2$Linux_X86_64 LibreOffice_...       \n",
       "{urn:oasis:names:tc:opendocument:xmlns:config:1...                                                NaN       \n",
       "{urn:oasis:names:tc:opendocument:xmlns:office:1...                                                NaN       \n",
       "{urn:oasis:names:tc:opendocument:xmlns:style:1....                                                NaN       \n",
       "\n",
       "                                                   {urn:oasis:names:tc:opendocument:xmlns:office:1.0}settings  \\\n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN           \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN           \n",
       "{urn:oasis:names:tc:opendocument:xmlns:config:1...  [{'{urn:oasis:names:tc:opendocument:xmlns:conf...           \n",
       "{urn:oasis:names:tc:opendocument:xmlns:office:1...                                                NaN           \n",
       "{urn:oasis:names:tc:opendocument:xmlns:style:1....                                                NaN           \n",
       "\n",
       "                                                   {urn:oasis:names:tc:opendocument:xmlns:office:1.0}scripts  \\\n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN          \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN          \n",
       "{urn:oasis:names:tc:opendocument:xmlns:config:1...                                                NaN          \n",
       "{urn:oasis:names:tc:opendocument:xmlns:office:1...  {'{http://openoffice.org/2004/office}libraries...          \n",
       "{urn:oasis:names:tc:opendocument:xmlns:style:1....                                                NaN          \n",
       "\n",
       "                                                   {urn:oasis:names:tc:opendocument:xmlns:office:1.0}font-face-decls  \\\n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN                  \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN                  \n",
       "{urn:oasis:names:tc:opendocument:xmlns:config:1...                                                NaN                  \n",
       "{urn:oasis:names:tc:opendocument:xmlns:office:1...                                                NaN                  \n",
       "{urn:oasis:names:tc:opendocument:xmlns:style:1....                           [None, None, None, None]                  \n",
       "\n",
       "                                                   {urn:oasis:names:tc:opendocument:xmlns:office:1.0}styles  \\\n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN         \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN         \n",
       "{urn:oasis:names:tc:opendocument:xmlns:config:1...                                                NaN         \n",
       "{urn:oasis:names:tc:opendocument:xmlns:office:1...                                                NaN         \n",
       "{urn:oasis:names:tc:opendocument:xmlns:style:1....                                                NaN         \n",
       "\n",
       "                                                   {urn:oasis:names:tc:opendocument:xmlns:office:1.0}automatic-styles  \\\n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN                   \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN                   \n",
       "{urn:oasis:names:tc:opendocument:xmlns:config:1...                                                NaN                   \n",
       "{urn:oasis:names:tc:opendocument:xmlns:office:1...                                                NaN                   \n",
       "{urn:oasis:names:tc:opendocument:xmlns:style:1....                                                NaN                   \n",
       "\n",
       "                                                   {urn:oasis:names:tc:opendocument:xmlns:office:1.0}master-styles  \\\n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN                \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN                \n",
       "{urn:oasis:names:tc:opendocument:xmlns:config:1...                                                NaN                \n",
       "{urn:oasis:names:tc:opendocument:xmlns:office:1...                                                NaN                \n",
       "{urn:oasis:names:tc:opendocument:xmlns:style:1....                                                NaN                \n",
       "\n",
       "                                                   {urn:oasis:names:tc:opendocument:xmlns:office:1.0}body  \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN      \n",
       "{urn:oasis:names:tc:opendocument:xmlns:meta:1.0...                                                NaN      \n",
       "{urn:oasis:names:tc:opendocument:xmlns:config:1...                                                NaN      \n",
       "{urn:oasis:names:tc:opendocument:xmlns:office:1...                                                NaN      \n",
       "{urn:oasis:names:tc:opendocument:xmlns:style:1....                                                NaN      "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the population_data.json file using pandas's \n",
    "# read_json method. Don't forget to specific the orient option\n",
    "# store the results in df_json\n",
    "\n",
    "df_json = pd.read_json('output.json',orient='records')\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Ways to Read in JSON\n",
    "\n",
    "Besides using pandas to read JSON files, you can use the json library. Python treats JSON data like a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# read the first record in the JSON file\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(json_data[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Assuming json_data is a dictionary, list the keys to understand its structure\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# read in the JSON file\n",
    "\n",
    "with open('output.json') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# read the first record in the JSON file\n",
    "print(json_data[0])\n",
    "print('\\n')\n",
    "# Assuming json_data is a dictionary, list the keys to understand its structure\n",
    "for key in json_data.keys():\n",
    "    print(f\"Key: {key}\")\n",
    "\n",
    "# Access a specific key, e.g., if \"data\" is a key within json_data\n",
    "# Adjust this key to match an actual top-level key in your JSON\n",
    "print(json_data[\"data\"])  # Replace \"data\" with the actual key you'd like to access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Extract XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lines(15, 'WDIEXCEL.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML appears almost exactly the same as HTML. XML is tag based and has value inside the tag JSON comes with simpler navigating compared to XML. Pandas cannot directly read the XML. The first one is that tag names are defined by you. The formatting of every XML file may be different. So, you can guess why XML more or less die compared to JSON.\n",
    "\n",
    "How to read XML, how to walk it\n",
    "\n",
    "Meals parsing of XML information turns out to be simpler with a Python library known as BeautifulSoup Read the Docs: Beautiful Soup docs\n",
    "\n",
    "The find() method will find the first occurrence of an xml element. Finding an example like find('record') will return the first record in the xml file:\n",
    "\n",
    "Aruba\n",
    "\n",
    "Population, total\n",
    "\n",
    "1960\n",
    "\n",
    "54211\n",
    "\n",
    "The find_all() method gives all the tags matched. Example: find_all('record') — this would return all of the `` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 1691891544: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# open the population_data.xml file and load into Beautiful Soup\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWDIEXCEL.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m----> 6\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(fp,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\__init__.py:314\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;241m=\u001b[39m parse_only\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(markup) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    316\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1691891544: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# import the BeautifulSoup library\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# open the population_data.xml file and load into Beautiful Soup\n",
    "with open(\"WDIEXCEL.xml\") as fp:\n",
    "    soup = BeautifulSoup(fp,\"lxml\") #lxml is the parser type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the first 5 records in the xml file\n",
    "# this is an example of how to navigate the XML document with BeautifulSoup\n",
    "\n",
    "i = 0\n",
    "# use the find_all method to get all record tags in the document\n",
    "for record in soup.find_all('record'):\n",
    "    # use the find_all method to get all fields in each record\n",
    "    i += 1\n",
    "    for record in record.find_all('field'):\n",
    "        print(record['name'], ': ' , record.text)\n",
    "    print()\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Extract Data from SQL Databases\n",
    "\n",
    "### Working with Pandas and `sqlite3`\n",
    "\n",
    "Pandas makes it easy to interact with SQL databases by enabling you to execute SQL queries and retrieve the results directly into a DataFrame. The approach you choose depends on the specific SQL database you're working with. For instance, you can use the built-in `sqlite3` library for SQLite databases, or you can use the more versatile `SQLAlchemy` library for a variety of relational databases, such as MySQL, PostgreSQL, and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been successfully converted to .db format.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Step 1: Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('Population.csv')\n",
    "\n",
    "# Step 2: Create or connect to a SQLite database\n",
    "conn = sqlite3.connect('population_data.db')\n",
    "\n",
    "# Step 3: Write the DataFrame to the SQLite database\n",
    "df.to_sql('population', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Step 4: Close the connection\n",
    "#conn.close()\n",
    "\n",
    "print(\"CSV file has been successfully converted to .db format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n"
     ]
    }
   ],
   "source": [
    "# Run a query to retrieve column names from the 'population' table\n",
    "cursor.execute(\"PRAGMA table_info(population);\")\n",
    "\n",
    "#  Fetch all column info\n",
    "columns = cursor.fetchall()\n",
    "#Extract and display the column names\n",
    "column_names = [column[1] for column in columns]\n",
    "print(\"Column names:\", column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Africa Eastern and Southern', 'AFE', 'Access to clean fuels and technologies for cooking (% of population)', 'EG.CFT.ACCS.ZS', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 11.5259044, 11.85059334, 12.17559206, 12.55172283, 12.92485126, 13.3477015, 13.80489275, 14.22628476, 14.64480146, 15.13687143, 15.59232816, 16.00535616, 16.46694456, 16.87731257, 17.40140968, 17.91123379, 18.46387394, 18.92403725, 19.4370542, 20.02625444, 20.64796871, 21.16587705, 21.8631387, None)\n",
      "('Africa Eastern and Southern', 'AFE', 'Access to clean fuels and technologies for cooking, rural (% of rural population)', 'EG.CFT.ACCS.RU.ZS', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 3.469977156, 3.642009674, 3.849941636, 4.05410797, 4.285291057, 4.490187073, 4.734117326, 4.996842243, 5.23900636, 5.457239938, 5.686399954, 5.959238619, 6.202220556, 6.453239693, 6.728819361, 7.005876551, 7.308571428, 7.547225991, 7.875917244, 8.243018269, 8.545483107, 8.906710921, 9.26131967, None)\n",
      "('Africa Eastern and Southern', 'AFE', 'Access to clean fuels and technologies for cooking, urban (% of urban population)', 'EG.CFT.ACCS.UR.ZS', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 32.43171442, 32.82868264, 33.2477393, 33.70205881, 34.19489547, 34.584696, 35.13673035, 35.69313109, 36.03878859, 36.5027965, 36.80532759, 37.16224347, 37.4859803, 37.78402885, 38.08093121, 38.42228186, 38.72210826, 38.99315729, 39.33787177, 39.69527932, 40.1378472, 40.5222092, 41.01113176, None)\n",
      "('Africa Eastern and Southern', 'AFE', 'Access to electricity (% of population)', 'EG.ELC.ACCS.ZS', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 19.96388176, 19.9691482, 21.58028243, 22.51696241, 23.75395284, 23.48737637, 25.19852692, 26.80728787, 25.94751303, 26.18591244, 27.40201436, 28.91159373, 31.66603842, 31.70305724, 31.86047402, 33.90380013, 38.85462442, 40.19989807, 43.01714798, 44.38125944, 46.26487456, 48.10086213, 48.71199459, None)\n",
      "('Africa Eastern and Southern', 'AFE', 'Access to electricity, rural (% of rural population)', 'EG.ELC.ACCS.RU.ZS', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 8.619522041, None, None, 9.753038578, 10.91845684, 10.41771633, 12.54004149, 12.53339255, 12.98231084, 15.52147468, 14.47103584, 16.10961696, 19.36955231, 18.68870749, 17.61947523, 16.50017058, 24.60586091, 25.39692869, 27.03752789, 29.13791374, 31.00104944, 32.77790964, 33.74790671, None)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a cursor object to execute SQL queries\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Step 3: Run a query to retrieve data (e.g., selecting all records from the 'population' table)\n",
    "cursor.execute(\"SELECT * FROM population LIMIT 5;\")  # Adjust the query as needed\n",
    "\n",
    "# Step 4: Fetch the results\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Step 5: Display the results\n",
    "for row in rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kenya', 'KEN', 'Access to clean fuels and technologies for cooking (% of population)', 'EG.CFT.ACCS.ZS', None)\n",
      "('Kenya', 'KEN', 'Access to clean fuels and technologies for cooking, rural (% of rural population)', 'EG.CFT.ACCS.RU.ZS', None)\n",
      "('Kenya', 'KEN', 'Access to clean fuels and technologies for cooking, urban (% of urban population)', 'EG.CFT.ACCS.UR.ZS', None)\n",
      "('Kenya', 'KEN', 'Access to electricity (% of population)', 'EG.ELC.ACCS.ZS', None)\n",
      "('Kenya', 'KEN', 'Access to electricity, rural (% of rural population)', 'EG.ELC.ACCS.RU.ZS', None)\n"
     ]
    }
   ],
   "source": [
    "# Example query: Retrieve all data for a specific country (e.g., 'United States')\n",
    "country_name = 'Kenya'\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT \"Country Name\", \"Country Code\", \"Indicator Name\", \"Indicator Code\", \"1960\"\n",
    "    FROM population\n",
    "    WHERE \"Country Name\" = ?\n",
    "    LIMIT 5\n",
    "\"\"\", (country_name,))\n",
    "\n",
    "\n",
    "\n",
    "# Fetch the results\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "#  Display the results\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLAlchemy and Pandas\n",
    "\n",
    "If you are working with a different type of database such as MySQL or PostgreSQL, you can use the SQLAlchemy library with pandas. Here are the instructions for connecting to [different types of databases using SQLAlchemy](http://docs.sqlalchemy.org/en/latest/core/engines.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "### \n",
    "# create a database engine \n",
    "# to find the correct file path, use the python os library:\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "#\n",
    "###\n",
    "\n",
    "# engine = create_engine('sqlite:////home/workspace/3_sql_exercise/population_data.db')\n",
    "# pd.read_sql(\"SELECT * FROM population_data\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## APIs \n",
    "\n",
    "### Example Indicators API\n",
    "\n",
    "Run the code example below to request data from the World Bank Indicators API. According to the documntation, you format your request url like so:\n",
    "\n",
    "`http://api.worldbank.org/v2/countries/` + list of country abbreviations separated by ; + `/indicators/` + indicator name + `?` + options\n",
    "\n",
    "where options can include\n",
    "* per_page - number of records to return per page\n",
    "* page - which page to return - eg if there are 5000 records and 100 records per page\n",
    "* date - filter by dates\n",
    "* format - json or xml\n",
    " \n",
    " and a few other options that you can read about [here](https://datahelpdesk.worldbank.org/knowledgebase/articles/898581-api-basic-call-structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'http://api.worldbank.org/v2/countries/br;cn;us;de/indicators/SP.POP.TOTL/?format=json&per_page=1000'\n",
    "r = requests.get(url)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This json data isn't quite ready for a pandas data frame. Notice that the json response is a list with two entries. The first entry is \n",
    "```\n",
    "{'lastupdated': '2018-06-28',\n",
    "  'page': 1,\n",
    "  'pages': 1,\n",
    "  'per_page': 1000,\n",
    "  'total': 232}\n",
    "```\n",
    "\n",
    "That first entry is meta data about the results. For example, it says that there is one page returned with 232 results. \n",
    "\n",
    "The second entry is another list containing the data. This data would need some cleaning to be used in a pandas data frame. That would happen later in the transformation step of an ETL pipeline. Run the cell below to read the results into a dataframe and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r.json()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some issues with this dataframe. The country and indicator variables don't look particularly useful in their current form. Again, dealing with those issues would come in the transformation phase of a pipeline.\n",
    "\n",
    "Use the Indicators API to request rural population data for Switzerland in the years 1995 through 2001. Here are a few helpful resources:\n",
    "* [documentation included how to filter by year](https://datahelpdesk.worldbank.org/knowledgebase/articles/898581-api-basic-call-structure)\n",
    "* [2-character iso country codes](https://www.nationsonline.org/oneworld/country_code_list.htm)\n",
    "* [search box for World Bank indicators](https://data.worldbank.org)\n",
    "\n",
    "To find the indicator code, first search for the indicator here: https://data.worldbank.org\n",
    "Click on the indicator name. The indicator code is in the url. For example, the indicator code for total population is SP.POP.TOTL, which you can see in the link [https://data.worldbank.org/indicator/SP.RUR.TOTL](https://data.worldbank.org/indicator/SP.RUR.TOTL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get the url ready\n",
    "url = 'http://api.worldbank.org/v2/country/CH/indicator/SP.POP.TOTL/?format=json&date=1995:2001'\n",
    "\n",
    "# TODO: send the request\n",
    "r = requests.get(url)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Combining Data\n",
    "\n",
    "There are two csv files:\n",
    "* rural_population_percent.csv\n",
    "* electricity_access_percent.csv\n",
    "\n",
    "They both come from the World Bank Indicators data. \n",
    "* https://data.worldbank.org/indicator/SP.RUR.TOTL.ZS\n",
    "* https://data.worldbank.org/indicator/EG.ELC.ACCS.ZS\n",
    "\n",
    "The rural populaton data represents the percent of a country's population that is rural over time. The electricity access data shows the percentage of people with access to electricity.\n",
    "\n",
    "I will combine these two data sets together into one pandas data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two data sets using the [pandas concat method](https://pandas.pydata.org/pandas-docs/stable/merging.html). In other words, find the union of the two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../input/world-bank-datasets/rural_population_percent.csv\")\n",
    "for i in range(10):\n",
    "    line = f.readline()\n",
    "    print('line: ', i, line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rural = pd.read_csv('../input/world-bank-datasets/rural_population_percent.csv',skiprows=4)\n",
    "df_rural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../input/world-bank-datasets/electricity_access_percent.csv\")\n",
    "for i in range(10):\n",
    "    line = f.readline()\n",
    "    print('line: ', i, line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_electricity = pd.read_csv('../input/world-bank-datasets/electricity_access_percent.csv',skiprows=4)\n",
    "df_electricity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove the `Unnamed:62` column from each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rural.drop(['Unnamed: 62'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_electricity.drop(['Unnamed: 62'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_rural, df_electricity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Cleaning Data\n",
    "\n",
    "Every data set might have its own issues whether that involves missing values, duplicated entries, data entry mistakes, etc. In this notebook, I'll do some data cleaning on the World Bank projects and World Bank indicators data sets.\n",
    "\n",
    "Currently, the projects data and the indicators data have different values for country names. My task in this notebook is to clean both data sets so that they have consistent country names. This will allow you to join the two data sets together. Cleaning data, unfortunately, can be tedious and take a lot of your time as a data scientist.\n",
    "\n",
    "Why might you want to join these data sets together? What if, for example, you wanted to run linear regression to try to predict project costs based on indicator data? Or you might want to analyze the types of projects that get approved versus the indicator data. For example, do countries with low rates of rural electrification have more rural themed projects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicator = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\n",
    "df_indicator.drop(['Unnamed: 62'], axis=1, inplace=True)\n",
    "\n",
    "# read in the projects data set with all columns type string\n",
    "df_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\n",
    "df_projects.drop(['Unnamed: 56'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell outputs the unique country names and ISO abbreviations in the population indicator data set. You'll notice a few values that represent world regions such as 'East Asia & Pacific' and 'East Asia & Pacific (excluding high income)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicator[['Country Name', 'Country Code']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to see the unique country names in the project data set. Notice that the projects data has two columns for country name. One is called 'countryname' and the other is called 'Country'. The 'Country' column only has NaN values.\n",
    "\n",
    "Another thing of note: It would've been easier to join the two data sets together if the projects data had the [ISO country abbreviations](https://en.wikipedia.org/wiki/ISO_3166-1) like the indicator data has. Unfortunately, the projects data does not have the ISO country abbreviations. To join these two data sets together, you essentially have two choices:\n",
    "* add a column of ISO 3 codes to the projects data set\n",
    "* find the difference between the projects data country names and indicator data country names. Then clean the data so that they are the same.\n",
    "\n",
    "Run the code cell below to see what the project countries look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects['countryname'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice a pattern in the projects data country names? The entries are repeated and separated by a semi-colon like this:\n",
    "```text\n",
    "'Kingdom of Spain;Kingdom of Spain'\n",
    "'New Zealand;New Zealand'\n",
    "```\n",
    "\n",
    "The first step is to clean the country name column and get rid of the semi-colon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects['Official Country Name'] = df_projects['countryname'].str.split(';').str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and import the pycountry library\n",
    "!pip install pycountry\n",
    "from pycountry import countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell to see an example of how the library works\n",
    "countries.get(name='Spain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell to see how you can also look up countries without specifying the key\n",
    "countries.lookup('Kingdom of Spain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to add the ISO codes to the projects data set. To start, use the pycountry library to make a dictionary mapping the unique countries in 'Official Country Name' to the ISO code.\n",
    "\n",
    "Iterate through the unique countries in df_projects['Official Country Name']. Create a dictionary mapping the 'Country Name' to the alpha_3 ISO abbreviations. \n",
    "\n",
    "The dictionary should look like:\n",
    "`{'Kingdom of Spain':'ESP'}`\n",
    "\n",
    "If a country name cannot be found in the pycountry library, add it to a list called `country_not_found`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the libraries and variables\n",
    "from collections import defaultdict\n",
    "country_not_found = [] # stores countries not found in the pycountry library\n",
    "project_country_abbrev_dict = defaultdict(str) # set up an empty dictionary of string values\n",
    "\n",
    "# iterate through the country names in df_projects. \n",
    "# Create a dictionary mapping the country name to the alpha_3 ISO code\n",
    "for country in df_projects['Official Country Name'].drop_duplicates().sort_values():\n",
    "    try: \n",
    "        # look up the country name in the pycountry library\n",
    "        # store the country name as the dictionary key and the ISO-3 code as the value\n",
    "        project_country_abbrev_dict[country] = countries.lookup(country).alpha_3\n",
    "    except:\n",
    "        # If the country name is not in the pycountry library, then print out the country name\n",
    "        # And store the results in the country_not_found list\n",
    "        print(country, ' not found')\n",
    "        country_not_found.append(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few country names were not in the pycountry library. Some of these are regions like \"South Asia\" or \"Southern Africa\", so it makes sense that these would not show up in the pycountry library.\n",
    "\n",
    "Part 3 - Making a Manual Mapping\n",
    "\n",
    "Perhaps some of these missing df_projects countries are already in the indicators data set. In the next cell, check if any of the countries in the country_not_found list are in the indicator list of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to iterate through the country_not_found list and check if the country name is in the df_indicator data set\n",
    "indicator_countries = df_indicator[['Country Name', 'Country Code']].drop_duplicates().sort_values(by='Country Name')\n",
    "\n",
    "for country in country_not_found:\n",
    "    if country in indicator_countries['Country Name'].tolist():\n",
    "        print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there aren't too many country names that match between df_indicator and df_projects. This is where data cleaning becomes especially tedious, but in this case, we've done a lot of the work for you.\n",
    "\n",
    "We've manually created a dictionary that maps all of the countries in country_not_found to the ISO-3 alpha codes. You **could** try to do this programatically using some sophisticated string matching rules. That might be worth your time for a larger data set. But in this case, it's probably faster to type out the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_not_found_mapping = {'Co-operative Republic of Guyana': 'GUY',\n",
    "             'Commonwealth of Australia':'AUS',\n",
    "             'Democratic Republic of Sao Tome and Prin':'STP',\n",
    "             'Democratic Republic of the Congo':'COD',\n",
    "             'Democratic Socialist Republic of Sri Lan':'LKA',\n",
    "             'East Asia and Pacific':'EAS',\n",
    "             'Europe and Central Asia': 'ECS',\n",
    "             'Islamic  Republic of Afghanistan':'AFG',\n",
    "             'Latin America':'LCN',\n",
    "              'Caribbean':'LCN',\n",
    "             'Macedonia':'MKD',\n",
    "             'Middle East and North Africa':'MEA',\n",
    "             'Oriental Republic of Uruguay':'URY',\n",
    "             'Republic of Congo':'COG',\n",
    "             \"Republic of Cote d'Ivoire\":'CIV',\n",
    "             'Republic of Korea':'KOR',\n",
    "             'Republic of Niger':'NER',\n",
    "             'Republic of Kosovo':'XKX',\n",
    "             'Republic of Rwanda':'RWA',\n",
    "              'Republic of The Gambia':'GMB',\n",
    "              'Republic of Togo':'TGO',\n",
    "              'Republic of the Union of Myanmar':'MMR',\n",
    "              'Republica Bolivariana de Venezuela':'VEN',\n",
    "              'Sint Maarten':'SXM',\n",
    "              \"Socialist People's Libyan Arab Jamahiriy\":'LBY',\n",
    "              'Socialist Republic of Vietnam':'VNM',\n",
    "              'Somali Democratic Republic':'SOM',\n",
    "              'South Asia':'SAS',\n",
    "              'St. Kitts and Nevis':'KNA',\n",
    "              'St. Lucia':'LCA',\n",
    "              'St. Vincent and the Grenadines':'VCT',\n",
    "              'State of Eritrea':'ERI',\n",
    "              'The Independent State of Papua New Guine':'PNG',\n",
    "              'West Bank and Gaza':'PSE',\n",
    "              'World':'WLD'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, update the project_country_abbrev_dict variable with these new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the project_country_abbrev_dict with the country_not_found_mapping dictionary\n",
    "# HINT: This is relatively straightforward. Python dictionaries have a method called update(), which essentially\n",
    "# appends a dictionary to another dictionary\n",
    "\n",
    "project_country_abbrev_dict.update(country_not_found_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5 - Make a 'Country Code' Column\n",
    "\n",
    "Next, create a 'Country Code' column in the data_projects data frame. Use the project_country_abbrev_dict and df_projects['Country Name'] column to create a new columns called 'Country 'Code'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the project_country_abbrev_dict and the df_projects['Country Name'] column to make a new column\n",
    "# of the alpha-3 country codes. This new column should be called 'Country Code'.\n",
    "\n",
    "# HINT: Use the apply method and a lambda function\n",
    "# HINT: The lambda function will use the project_country_abbrev_dict that maps the country name to the ISO code\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html\n",
    "\n",
    "df_projects['Country Code'] = df_projects['Official Country Name'].apply(lambda x: project_country_abbrev_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell to see which projects in the df_projects data frame still have no country code abbreviation.\n",
    "# In other words, these projects do not have a matching population value in the df_indicator data frame.\n",
    "df_projects[df_projects['Country Code'] == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the df_projects dataframe and the df_indicator dataframe have a matching column called 'Country Code'. But these two data frames can't be merged quite yet. \n",
    "\n",
    "Each project in the df_projects dataframe also has a date associated with it. The idea would be to merge the df_projects dataframe with the df_indicator dataframe so that each project also had a population value associated with it. There are still more data transformations to do in order for that to be possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Data Types\n",
    "\n",
    "When reading in a data set, pandas will try to guess the data type of each column like float, integer, datettime, bool, etc. In Pandas, strings are called \"object\" dtypes. \n",
    "\n",
    "However, Pandas does not always get this right. That was the issue with the World Bank projects data. Hence, the dtype was specified as a string:\n",
    "```\n",
    "df_projects = pd.read_csv('../data/projects_data.csv', dtype=str)\n",
    "```\n",
    "\n",
    "Run the code cells below to read in the indicator and projects data. Then run the following code cell to see the dtypes of the indicator data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the population data and drop the final column\n",
    "df_indicator = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\n",
    "df_indicator.drop(['Unnamed: 62'], axis=1, inplace=True)\n",
    "\n",
    "# read in the projects data set with all columns type string\n",
    "df_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\n",
    "df_projects.drop(['Unnamed: 56'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicator.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look reasonable. Country Name, Country Code, Indicator Name and Indicator Code were all read in as strings. The year columns, which contain the population data, were read in as floats.\n",
    "\n",
    "Since the population indicator data was read in correctly, you can run calculations on the data. In this first exercise, sum the populations of the United States, Canada, and Mexico by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the population sum by year for Canada,\n",
    "#       the United States, and Mexico.\n",
    "\n",
    "# the keepcol variable makes a list of the column names to keep. You can use this if you'd like\n",
    "keepcol = ['Country Name']\n",
    "for i in range(1960, 2018, 1):\n",
    "    keepcol.append(str(i))\n",
    "\n",
    "# In the df_nafta variable, store a data frame that only contains the rows for \n",
    "#      Canada, United States, and Mexico.\n",
    "df_nafta = df_indicator[(df_indicator['Country Name'] == 'Canada') | \n",
    "             (df_indicator['Country Name'] == 'United States') | \n",
    "            (df_indicator['Country Name'] == 'Mexico')].iloc[:,]\n",
    "\n",
    "\n",
    "# Calculate the sum of the values in each column in order to find the total population by year.\n",
    "# You can use the keepcol variable if you want to control which columns get outputted\n",
    "df_nafta.sum(axis=0)[keepcol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the code cell below to look at the dtypes for the projects data set. They should all be \"object\" types, ie strings, because that's what was specified in the code when reading in the csv file. As a reminder, this was the code:\n",
    "```\n",
    "df_projects = pd.read_csv('../data/projects_data.csv', dtype=str)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these columns should be strings, so there's no problem; however, a few columns should be other data types. For example, `boardapprovaldate` should be a datettime and `totalamt` should be an integer. You'll learn about datetime formatting in the next part of the lesson. For this exercise, focus on the 'totalamt' and 'lendprojectcost' columns. Run the code cell below to see what that data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects[['totalamt', 'lendprojectcost']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects['totalamt'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened? Pandas treated the totalamts like strings. In Python, adding strings concatenates the strings together.\n",
    "\n",
    "There are a few ways to remedy this. When using pd.read_csv(), you could specify the column type for every column in the data set. The pd.read_csv() dtype option can accept a dictionary mapping each column name to its data type. You could also specify the `thousands` option with `thousands=','`. This specifies that thousands are separated by a comma in this data set. \n",
    "\n",
    "However, this data is somewhat messy, contains missing values, and has a lot of columns. It might be faster to read in the entire data set with string types and then convert individual columns as needed. For this next exercise, convert the `totalamt` column from a string to an integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the totalamt column from a string to a float and save the results back into the totalamt column\n",
    "\n",
    "# Step 1: Remove the commas from the 'totalamt' column\n",
    "# HINT: https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.str.replace.html\n",
    "\n",
    "# Step 2: Convert the 'totalamt' column from an object data type (ie string) to an integer data type.\n",
    "# HINT: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.to_numeric.html\n",
    "\n",
    "df_projects['totalamt'] = pd.to_numeric(df_projects['totalamt'].str.replace(',',\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With messy data, you might find it easier to read in everything as a string; however, you'll sometimes have to convert those strings to more appropriate data types. When you output the dtypes of a dataframe, you'll generally see these values in the results:\n",
    "* float64\n",
    "* int64\n",
    "* bool\n",
    "* datetime64\n",
    "* timedelta\n",
    "* object\n",
    "\n",
    "where timedelta is the difference between two datetimes and object is a string. As you've seen here, you sometimes need to convert data types from one type to another type. Pandas has a few different methods for converting between data types, and here are link to the documentation:\n",
    "\n",
    "* [astype](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.astype.html#pandas.DataFrame.astype)\n",
    "* [to_datetime](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_datetime.html#pandas.to_datetime)\n",
    "* [to_numeric](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_numeric.html#pandas.to_numeric)\n",
    "* [to_timedelta](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_timedelta.html#pandas.to_timedelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Parsing Dates\n",
    "\n",
    "Another common data transformation involves parsing dates. Parsing generally means that you start with a string and then transform that string into a different data type. In this case, that means taking a date in the format of a string and transforming the string into a date type. Run the next cell to see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_date = pd.to_datetime('January 1st, 2017')\n",
    "parsed_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_date.second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes date string are formatted in unexpected ways. For example, in the United States, dates are given with the month first and then the day. That is what pandas expects by default. However, some countries write the date with the day first and then the month. Run the next three examples to see Panda's default behavior and how you can specify the date formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_date = pd.to_datetime('5/3/2017 5:30')\n",
    "parsed_date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_date = pd.to_datetime('3/5/2017 5:30', format='%d/%m/%Y %H:%M')\n",
    "parsed_date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_date = pd.to_datetime('5/3/2017 5:30', format='%m/%d/%Y %H:%M')\n",
    "parsed_date.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formatting abbreviations are actually part of the python standard. You can see examples at [this link](http://strftime.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the projects data set with all columns type string\n",
    "df_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\n",
    "df_projects.drop(['Unnamed: 56'], axis=1, inplace=True)\n",
    "df_projects.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there are three columns associated with dates: boardapprovaldate, board_approval_month, and closingdate. Run the code cell below to see what these values look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects.head(15)[['boardapprovaldate', 'board_approval_month', 'closingdate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pandas to_datetime method to convert the boardapprovaldate and closingdate columns into datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pandas to_datetime method to convert these two columns \n",
    "#   (boardapprovaldate, closingdate) into date times.\n",
    "# HINT: It's easier to do this one column at a time\n",
    "\n",
    "df_projects['boardapprovaldate'] = pd.to_datetime(df_projects['boardapprovaldate'])\n",
    "df_projects['closingdate'] = pd.to_datetime(df_projects['closingdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code cells below to see how you can access the different parts of the datetime objects\n",
    "# Series.dt gives access to the datetime object as explained here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.html\n",
    "df_projects['boardapprovaldate'].dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell to see the output\n",
    "df_projects['boardapprovaldate'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to see the output\n",
    "# weekday represents the day of the week from 0 (Monday) to 6 (Sunday).\n",
    "df_projects['boardapprovaldate'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 - Create new columns\n",
    "\n",
    "Now that the boardapprovaldate and closingdates are in datetime formats, create a few new columns in the df_projects data frame:\n",
    "* approvalyear\n",
    "* approvalday\n",
    "* approvalweekday\n",
    "* closingyear\n",
    "* closingday\n",
    "* closingweekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# create the follwing new columns in the df_projects data frame\n",
    "#\n",
    "# approvalyear\n",
    "# approvalday\n",
    "# approvalweekday\n",
    "# closingyear\n",
    "# closingday\n",
    "# closingweekday\n",
    "#\n",
    "#\n",
    "###\n",
    "\n",
    "df_projects['approvalyear'] = df_projects['boardapprovaldate'].dt.year\n",
    "df_projects['approvalday'] = df_projects['boardapprovaldate'].dt.day\n",
    "df_projects['approvalweekday'] = df_projects['boardapprovaldate'].dt.weekday\n",
    "df_projects['closingyear'] = df_projects['closingdate'].dt.year\n",
    "df_projects['closingday'] = df_projects['closingdate'].dt.day\n",
    "df_projects['closingweekday'] = df_projects['closingdate'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Encodings\n",
    "\n",
    "Encodings are a set of rules mapping string characters to their binary representations. Python supports dozens of different encoding as seen here in [this link](https://docs.python.org/3/library/codecs.html#standard-encodings). Because the web was originally in English, the first encoding rules mapped binary code to the English alphabet. \n",
    "\n",
    "The English alphabet has only 26 letters. But other languages have many more characters including accents, tildes and umlauts. As time went on, more encodings were invented to deal with languages other than English. The utf-8 standard tries to provide a single encoding schema that can encompass all text.\n",
    "\n",
    "The problem is that it's difficult to know what encoding rules were used to make a file unless somebody tells you. The most common encoding by far is utf-8. Pandas will assume that files are utf-8 when you read them in or write them out.\n",
    "\n",
    "Run the code cell below to read in the population data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas should have been able to read in this data set without any issues. Next, run the code cell below to read in the 'mystery.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/world-bank-datasets/mystery.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have gotten an error: **UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte**. This means pandas assumed the file had a utf-8 encoding but had trouble reading in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodings.aliases import aliases\n",
    "\n",
    "alias_values = set(aliases.values())\n",
    "\n",
    "for encoding in set(aliases.values()):\n",
    "    try:\n",
    "        df=pd.read_csv(\"mystery.csv\", encoding=encoding)\n",
    "        print('successful', encoding)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are dozens of encodings that Python can handle; however, Pandas assumes a utf-8 encoding. This makes sense since utf-8 is very common. However, you will sometimes come across files with other encodings. If you don't know the encoding, you have to search for it.\n",
    "\n",
    "There is a Python library that can be of some help when you don't know an encoding: chardet. Run the code cells below to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the chardet library\n",
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the chardet library\n",
    "import chardet \n",
    "\n",
    "# use the detect method to find the encoding\n",
    "# 'rb' means read in the file as binary\n",
    "with open(\"../input/world-bank-datasets/mystery.csv\", 'rb') as file:\n",
    "    print(chardet.detect(file.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a machine learning algorithm won't work with missing values. This is essentially correct; however, there are a couple of situations where this isn't quite true. For example, if you had a categorical variable, you could keep the NULL value as one of the options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Imputing Data\n",
    "\n",
    "When a dataset has missing values, you can either remove those values or fill them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\n",
    "df.drop('Unnamed: 62', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code cell to see what the data looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell to check how many null values are in the data set\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few null values. Run the code below to plot the data for a few countries in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# put the data set into long form instead of wide\n",
    "df_melt = pd.melt(df, id_vars=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'], var_name='year', value_name='GDP')\n",
    "\n",
    "# convert year to a date time\n",
    "df_melt['year'] = pd.to_datetime(df_melt['year'])\n",
    "\n",
    "def plot_results(column_name):\n",
    "    # plot the results for Afghanistan, Albania, and Honduras\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    df_melt[(df_melt['Country Name'] == 'Afghanistan') | \n",
    "            (df_melt['Country Name'] == 'Albania') | \n",
    "            (df_melt['Country Name'] == 'Honduras')].groupby('Country Name').plot('year', column_name, legend=True, ax=ax)\n",
    "    ax.legend(labels=['Afghanistan', 'Albania', 'Honduras'])\n",
    "    \n",
    "plot_results('GDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afghanistan and Albania are missing data, which show up as gaps in the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Exercise - Part 1\n",
    "\n",
    "Your first task is to calculate mean GDP for each country and fill in missing values with the country mean. This is a bit tricky to do in pandas. Here are a few links that should be helpful:\n",
    "* https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.groupby.html\n",
    "* https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.transform.html\n",
    "* https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.fillna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the df_melt dataframe and fill in missing values with a country's mean GDP\n",
    "# If you aren't sure how to do this, \n",
    "# look up something like \"how to group data and fill in nan values in pandas\" in a search engine\n",
    "# Put the results in a new column called 'GDP_filled'.\n",
    "\n",
    "# HINT: You can do this with these methods: groupby(), transform(), a lambda function, fillna(), and mean()\n",
    "\n",
    "df_melt['GDP_filled'] = df_melt.groupby('Country Name')['GDP'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_results('GDP_filled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somewhat of an improvement. At least there is no missing data; however, because GDP tends to increase over time, the mean GDP is probably not the best way to fill in missing values for this particular case. Next, try using forward fill to deal with any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise - Part 2\n",
    "\n",
    "Use the fillna forward fill method to fill in the missing data. Here is the [documentation](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.fillna.html).\n",
    "\n",
    "The pandas fillna method has a forward fill option. For example, if you wanted to use forward fill on the GDP dataset, you could execute `df_melt['GDP'].fillna(method='ffill')`. However, there are two issues with that code. \n",
    "1. You want to first make sure the data is sorted by year\n",
    "2. You need to group the data by country name so that the forward fill stays within each country\n",
    "\n",
    "Write code to first sort the df_melt dataframe by year, then group by 'Country Name', and finally use the forward fill method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use forward fill to fill in missing GDP values\n",
    "# HINTS: use the sort_values(), groupby(), and fillna() methods\n",
    "\n",
    "df_melt['GDP_ffill'] = df_melt.sort_values('year').groupby('Country Name')['GDP'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plot_results('GDP_ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better at least for the Afghanistan data; however, the Albania data is still missing values. You can fill in the Albania data using back fill. That is what you'll do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise - Part 3\n",
    "\n",
    "This part is similar to Part 2, but now you will use backfill. Write code that backfills the missing GDP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use back fill to fill in missing GDP values\n",
    "# HINTS: use the sort_values(), groupby(), and fillna() methods\n",
    "\n",
    "df_melt['GDP_bfill'] = df_melt.sort_values('year').groupby('Country Name')['GDP'].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results('GDP_bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the GDP data for all three countries is now complete. Note that forward fill did not fill all the Albania data because the first data entry in 1960 was NaN. Forward fill would try to fill the 1961 value with the NaN value from 1960.\n",
    "\n",
    "To completely fill the entire GDP data for all countries, you might have to run both forward fill and back fill. Note as well that the results will be slightly different depending on if you run forward fill first or back fill first. Afghanistan, for example, is missing data in the middle of the data set. Hence forward fill and back fill will have slightly different results.\n",
    "\n",
    "Run this next code cell to see if running both forward fill and back fill end up filling all the GDP NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward fill and backward fill on the GDP data\n",
    "df_melt['GDP_ff_bf'] = df_melt.sort_values('year').groupby('Country Name')['GDP'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Check if any GDP values are null\n",
    "df_melt['GDP_ff_bf'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Duplicate Data\n",
    "\n",
    "A data set might have duplicate data: in other words, the same record is represented multiple times. Sometimes, it's easy to find and eliminate duplicate data like when two records are exactly the same. At other times, like what was discussed in the video, duplicate data is hard to spot. \n",
    "\n",
    "From the World Bank GDP data, count the number of countries that have had a project totalamt greater than 1 billion dollars (1,000,000,000). To get the count, you'll have to remove duplicate data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\n",
    "projects.drop('Unnamed: 56', axis=1, inplace=True)\n",
    "projects['totalamt'] = pd.to_numeric(projects['totalamt'].str.replace(',', ''))\n",
    "projects['countryname'] = projects['countryname'].str.split(';', expand=True)[0]\n",
    "projects['boardapprovaldate'] = pd.to_datetime(projects['boardapprovaldate'])\n",
    "\n",
    "# filter the data frame for projects over 1 billion dollars\n",
    "\n",
    "# count the number of unique countries in the results\n",
    "\n",
    "projects[projects['totalamt'] > 1000000000]['countryname'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Dummy Variables\n",
    "\n",
    "In this exercise, I'll create dummy variables from the projects data set. The idea is to transform categorical data like this:\n",
    "\n",
    "| Project ID | Project Category |\n",
    "|------------|------------------|\n",
    "| 0          | Energy           |\n",
    "| 1          | Transportation   |\n",
    "| 2          | Health           |\n",
    "| 3          | Employment       |\n",
    "\n",
    "into new features that look like this:\n",
    "\n",
    "| Project ID | Energy | Transportation | Health | Employment |\n",
    "|------------|--------|----------------|--------|------------|\n",
    "| 0          | 1      | 0              | 0      | 0          |\n",
    "| 1          | 0      | 1              | 0      | 0          |\n",
    "| 2          | 0      | 0              | 1      | 0          |\n",
    "| 3          | 0      | 0              | 0      | 1          |\n",
    "\n",
    "\n",
    "(Note if you were going to use this data with a model influenced by multicollinearity, you would want to eliminate one of the columns to avoid redundant information.) \n",
    "\n",
    "The reasoning behind these transformations is that machine learning algorithms read in numbers not text. Text needs to be converted into numbers. You could assign a number to each category like 1, 2, 3, and 4. But a categorical variable has no inherent order, so you want to reflect this in your features.\n",
    "\n",
    "Pandas makes it very easy to create dummy variables with the [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) method. In this exercise, you'll create dummy variables from the World Bank projects data; however, there's a caveat. The World Bank data is not particularly clean, so you'll need to explore and wrangle the data first.\n",
    "\n",
    "You'll focus on the text values in the sector variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\n",
    "projects.drop('Unnamed: 56', axis=1, inplace=True)\n",
    "projects['totalamt'] = pd.to_numeric(projects['totalamt'].str.replace(',', ''))\n",
    "projects['countryname'] = projects['countryname'].str.split(';', expand=True)[0]\n",
    "projects['boardapprovaldate'] = pd.to_datetime(projects['boardapprovaldate'])\n",
    "\n",
    "# keep the project name, lending, sector and theme data\n",
    "sector = projects.copy()\n",
    "sector = sector[['project_name', 'lendinginstr', 'sector1', 'sector2', 'sector3', 'sector4', 'sector5', 'sector',\n",
    "          'mjsector1', 'mjsector2', 'mjsector3', 'mjsector4', 'mjsector5',\n",
    "          'mjsector', 'theme1', 'theme2', 'theme3', 'theme4', 'theme5', 'theme ',\n",
    "          'goal', 'financier', 'mjtheme1name', 'mjtheme2name', 'mjtheme3name',\n",
    "          'mjtheme4name', 'mjtheme5name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below. This cell shows the percentage of each variable that is null. Notice the mjsector1 through mjsector5 variables are all null. The mjtheme1name through mjtheme5name are also all null as well as the theme variable. \n",
    "\n",
    "Because these variables contain so many null values, they're probably not very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output percentage of values that are missing\n",
    "100 * sector.isnull().sum() / sector.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sector1 variable looks promising; it doesn't contain any null values at all. In the next cell, store the unique sector1 values in a list and output the results. Use the sort_values() and unique() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the unique values in sector1. Use the sort_values() and unique() pandas methods. \n",
    "# And then convert those results into a Python list\n",
    "uniquesectors1 = sector['sector1'].sort_values().unique()\n",
    "uniquesectors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code cell to see the number of unique values\n",
    "print('Number of unique values in sector1:', len(uniquesectors1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few issues with this 'sector1' variable. First, there are values labeled '!$!0'. These should be substituted with NaN.\n",
    "\n",
    "Furthermore, each sector1 value ends with a ten or eleven character string like '!$!49!$!EP'. Some sectors show up twice in the list like:\n",
    " 'Other Industry; Trade and Services!$!70!$!YZ',\n",
    " 'Other Industry; Trade and Services!$!63!$!YZ',\n",
    "\n",
    "But it seems like those are actually the same sector. You'll need to remove everything past the exclamation point. \n",
    "\n",
    "Many values in the sector1 variable start with the term '(Historic)'. Try removing that phrase as well.\n",
    "\n",
    "### replace() method\n",
    "\n",
    "With pandas, you can use the replace() method to search for text and replace parts of a string with another string. If you know the exact string you're looking for, the replace() method is straight forward. For example, say you wanted to remove the string '(Trial)' from this data:\n",
    "\n",
    "| data                     |\n",
    "|--------------------------|\n",
    "| '(Trial) Banking'        |\n",
    "| 'Banking'                |\n",
    "| 'Farming'                |\n",
    "| '(Trial) Transportation' |\n",
    "\n",
    "You could use `df['data'].replace('(Trial'), '')` to replace (Trial) with an empty string.\n",
    "\n",
    "### regular expressions\n",
    "What about this data?\n",
    "\n",
    "| data                                           |\n",
    "|------------------------------------------------|\n",
    "| 'Other Industry; Trade and Services?$ab' |\n",
    "| 'Other Industry; Trade and Services?ceg' |\n",
    "\n",
    "This type of data is trickier. In this case, there's a pattern where you want to remove a string that starts with an exclamation point and then has an unknown number of characters after it. When you need to match patterns of character, you can use [regular expressions](https://en.wikipedia.org/wiki/Regular_expression).\n",
    "\n",
    "The replace method can take a regular expression. So\n",
    "df['data'].replace('?.+', regex=True) where '?.+' means find a set of characters that starts with a question mark is then followed by one or more characters. You can see a [regular expression cheat sheet](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285) here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In the sector1 variable, replace the string '!$10' with nan\n",
    "# HINT: you can use the pandas replace() method and numpy.nan\n",
    "sector['sector1'] = sector['sector1'].replace('!$!0', np.nan)\n",
    "\n",
    "# TODO: In the sector1 variable, remove the last 10 or 11 characters from the sector1 variable.\n",
    "# HINT: There is more than one way to do this including the replace method\n",
    "# HINT: You can use a regex expression '!.+'\n",
    "# That regex expression looks for a string with an exclamation\n",
    "# point followed by one or more characters\n",
    "\n",
    "sector['sector1'] = sector['sector1'].replace('!.+', '', regex=True)\n",
    "\n",
    "# TODO: Remove the string '(Historic)' from the sector1 variable\n",
    "# HINT: You can use the replace method\n",
    "sector['sector1'] = sector['sector1'].replace('^(\\(Historic\\))', '', regex=True)\n",
    "\n",
    "print('Number of unique sectors after cleaning:', len(list(sector['sector1'].unique())))\n",
    "print('Percentage of null values after cleaning:', 100 * sector['sector1'].isnull().sum() / sector['sector1'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are 156 unique categorical values. That's better than 3060. If you were going to use this data with a supervised learning machine model, you could try converting these 156 values to dummy variables. You'd still have to train and test a model to see if those are good features.\n",
    "\n",
    "In this next exercise, use the pandas pd.get_dummies() method to create dummy variables. Then use the concat() method to concatenate the dummy variables to a dataframe that contains the project totalamt variable and the project year from the boardapprovaldate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.DataFrame(pd.get_dummies(sector['sector1']))\n",
    "\n",
    "#  Filter the projects data for the totalamt, the year from boardapprovaldate, and the dummy variables\n",
    "projects['year'] = projects['boardapprovaldate'].dt.year\n",
    "df = projects[['totalamt','year']]\n",
    "df_final = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Finding Outliers\n",
    "\n",
    "In this, I'll practice looking for outliers. I'll look at the World Bank GDP and population data sets. First, you'll look at the data from a one-dimensional perspective and then a two-dimensional perspective.\n",
    "\n",
    "Run the code below to import the data sets and prepare the data for analysis. The code:\n",
    "* reads in the data sets\n",
    "* reshapes the datasets to a long format\n",
    "* uses back fill and forward fill to fill in missing values\n",
    "* merges the gdp and population data together\n",
    "* shows the first 10 values in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the projects data set and do basic wrangling \n",
    "gdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\n",
    "gdp.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "population = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\n",
    "population.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Reshape the data sets so that they are in long format\n",
    "gdp_melt = gdp.melt(id_vars=['Country Name'], \n",
    "                    var_name='year', \n",
    "                    value_name='gdp')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing gdp values\n",
    "gdp_melt['gdp'] = gdp_melt.sort_values('year').groupby('Country Name')['gdp'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "population_melt = population.melt(id_vars=['Country Name'], \n",
    "                                  var_name='year', \n",
    "                                  value_name='population')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing population values\n",
    "population_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# merge the population and gdp data together into one data frame\n",
    "df_country = gdp_melt.merge(population_melt, on=('Country Name', 'year'))\n",
    "\n",
    "# filter data for the year 2016\n",
    "df_2016 = df_country[df_country['year'] == '2016']\n",
    "\n",
    "# see what the data looks like\n",
    "df_2016.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data set to identify outliers using the Tukey rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Make a boxplot of the population data for the year 2016\n",
    "df_2016.plot('population',kind='box');\n",
    "\n",
    "# Make a boxplot of the gdp data for the year 2016\n",
    "df_2016.plot('gdp',kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Tukey rule to determine what values of the population data are outliers for the year 2016. The Tukey rule finds outliers in one-dimension. The steps are:\n",
    "\n",
    "* Find the first quartile (ie .25 quantile)\n",
    "* Find the third quartile (ie .75 quantile)\n",
    "* Calculate the inter-quartile range (Q3 - Q1)\n",
    "* Any value that is greater than Q3 + 1.5 * IQR is an outlier\n",
    "* Any value that is less than Qe - 1.5 * IQR is an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_2016 = df_2016[['Country Name','population']]\n",
    "\n",
    "# Calculate the first quartile of the population values for 2016\n",
    "# HINT: you can use the pandas quantile method \n",
    "Q1 = population_2016['population'].quantile(0.25)\n",
    "\n",
    "# Calculate the third quartile of the population values for 2016\n",
    "Q3 = population_2016['population'].quantile(0.75)\n",
    "\n",
    "# Calculate the interquartile range Q3 - Q1\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate the maximum value and minimum values according to the Tukey rule\n",
    "# max_value is Q3 + 1.5 * IQR while min_value is Q1 - 1.5 * IQR\n",
    "max_value = Q3 + 1.5 * IQR\n",
    "min_value = Q1 - 1.5 * IQR\n",
    "\n",
    "# filter the population_2016 data for population values that are greater than max_value or less than min_value\n",
    "population_outliers = population_2016[(population_2016['population'] > max_value) | (population_2016['population'] < min_value)]\n",
    "population_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly many of these outliers are due to regional data getting aggregated together. \n",
    "\n",
    "Remove these data points and redo the analysis. There's a list provided below of the 'Country Name' values that are not actually countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows from the data that have Country Name values in the non_countries list\n",
    "# Store the filter results back into the df_2016 variable\n",
    "\n",
    "non_countries = ['World',\n",
    " 'High income',\n",
    " 'OECD members',\n",
    " 'Post-demographic dividend',\n",
    " 'IDA & IBRD total',\n",
    " 'Low & middle income',\n",
    " 'Middle income',\n",
    " 'IBRD only',\n",
    " 'East Asia & Pacific',\n",
    " 'Europe & Central Asia',\n",
    " 'North America',\n",
    " 'Upper middle income',\n",
    " 'Late-demographic dividend',\n",
    " 'European Union',\n",
    " 'East Asia & Pacific (excluding high income)',\n",
    " 'East Asia & Pacific (IDA & IBRD countries)',\n",
    " 'Euro area',\n",
    " 'Early-demographic dividend',\n",
    " 'Lower middle income',\n",
    " 'Latin America & Caribbean',\n",
    " 'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    " 'Latin America & Caribbean (excluding high income)',\n",
    " 'Europe & Central Asia (IDA & IBRD countries)',\n",
    " 'Middle East & North Africa',\n",
    " 'Europe & Central Asia (excluding high income)',\n",
    " 'South Asia (IDA & IBRD)',\n",
    " 'South Asia',\n",
    " 'Arab World',\n",
    " 'IDA total',\n",
    " 'Sub-Saharan Africa',\n",
    " 'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    " 'Sub-Saharan Africa (excluding high income)',\n",
    " 'Middle East & North Africa (excluding high income)',\n",
    " 'Middle East & North Africa (IDA & IBRD countries)',\n",
    " 'Central Europe and the Baltics',\n",
    " 'Pre-demographic dividend',\n",
    " 'IDA only',\n",
    " 'Least developed countries: UN classification',\n",
    " 'IDA blend',\n",
    " 'Fragile and conflict affected situations',\n",
    " 'Heavily indebted poor countries (HIPC)',\n",
    " 'Low income',\n",
    " 'Small states',\n",
    " 'Other small states',\n",
    " 'Not classified',\n",
    " 'Caribbean small states',\n",
    " 'Pacific island small states']\n",
    "\n",
    "# remove non countries from the data\n",
    "df_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rerun the Tukey code with this filtered data to find population outliers\n",
    "\n",
    "# Filter the data for the year 2016 and put the results in the population_2016 variable. You only need\n",
    "# to keep the Country Name and population columns\n",
    "population_2016 = df_2016[['Country Name','population']]\n",
    "\n",
    "# Calculate the first quartile of the population values\n",
    "# HINT: you can use the pandas quantile method \n",
    "Q1 = population_2016['population'].quantile(0.25)\n",
    "\n",
    "# Calculate the third quartile of the population values\n",
    "Q3 = population_2016['population'].quantile(0.75)\n",
    "\n",
    "# Calculate the interquartile range Q3 - Q1\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate the maximum value and minimum values according to the Tukey rule\n",
    "# max_value is Q3 + 1.5 * IQR while min_value is Q1 - 1.5 * IQR\n",
    "max_value = Q3 + 1.5 * IQR\n",
    "min_value = Q1 - 1.5 * IQR\n",
    "\n",
    "# filter the population_2016 data for population values that are greater than max_value or less than min_value\n",
    "population_outliers = population_2016[(population_2016['population'] > max_value) | (population_2016['population'] < min_value)]\n",
    "population_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the year 2016 and put the results in the population_2016 variable. You only need\n",
    "# to keep the Country Name and population columns\n",
    "gdp_2016 = df_2016[['Country Name','gdp']]\n",
    "\n",
    "# Calculate the first quartile of the population values\n",
    "# HINT: you can use the pandas quantile method \n",
    "Q1 = gdp_2016['gdp'].quantile(0.25)\n",
    "\n",
    "# Calculate the third quartile of the population values\n",
    "Q3 = gdp_2016['gdp'].quantile(0.75)\n",
    "\n",
    "# Calculate the interquartile range Q3 - Q1\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate the maximum value and minimum values according to the Tukey rule\n",
    "# max_value is Q3 + 1.5 * IQR while min_value is Q1 - 1.5 * IQR\n",
    "max_value = Q3 + 1.5 * IQR\n",
    "min_value = Q1 - 1.5 * IQR\n",
    "\n",
    "# filter the population_2016 data for population values that are greater than max_value or less than min_value\n",
    "gdp_outliers = gdp_2016[(gdp_2016['gdp'] > max_value) | (gdp_2016['gdp'] < min_value)]\n",
    "gdp_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write code to determine which countries are in the population_outliers array and in the gdp_outliers array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find country names that are in both the population_outliers and the gdp_outliers \n",
    "# HINT: you can use the pandas intersection() method and python set() and list() methods\n",
    "\n",
    "list(set(population_outliers['Country Name']).intersection(gdp_outliers['Country Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These countries have both relatively high populations and high GDPs. That might be an indication that although these countries have high values for both gdp and population, they're not true outliers when looking at these values from a two-dimensional perspective.\n",
    "\n",
    "Now write code to find countries in population_outliers but not in the gdp_outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find country names that are in the population outliers list but not the gdp outliers list\n",
    "# HINT: Python's set() and list() methods should be helpful\n",
    "\n",
    "list(set(population_outliers['Country Name']) - set(gdp_outliers['Country Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These countries are population outliers but not GDP outliers. If looking at outliers from a two-dimensional perspective, there's some indication that these countries might be outliers.\n",
    "\n",
    "And finally, write code to find countries that are in the gdp_outliers array but not the population_outliers array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find country names that are in the gdp outliers list but not the population outliers list\n",
    "# HINT: Python's set() and list() methods should be helpful\n",
    "\n",
    "list(set(gdp_outliers['Country Name']) - set(population_outliers['Country Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, these countries have high GDP but are not population outliers.\n",
    "\n",
    "\n",
    "2-Dimensional Analysis\n",
    "\n",
    "Next, look at the data from a two-dimensional perspective.\n",
    "\n",
    "The next code cell plots the GDP vs Population data including the country name of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(df_2016['population'])\n",
    "y = list(df_2016['gdp'])\n",
    "text = df_2016['Country Name']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(x, y)\n",
    "plt.title('GDP vs Population')\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('GDP')\n",
    "for i, txt in enumerate(text):\n",
    "    ax.annotate(txt, (x[i],y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The United States, China, and India have such larger values that it's hard to see this data. Let's take those countries out for a moment and look at the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_large = (df_2016['Country Name'] != 'United States') & (df_2016['Country Name'] != 'India') & (df_2016['Country Name'] != 'China')\n",
    "x = list(df_2016[df_no_large]['population'])\n",
    "y = list(df_2016[df_no_large]['gdp'])\n",
    "text = df_2016[df_no_large]['Country Name']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(x, y)\n",
    "plt.title('GDP vs Population')\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('GDP')\n",
    "for i, txt in enumerate(text):\n",
    "    ax.annotate(txt, (x[i],y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit a linear regression model on the population and gdp data\n",
    "model = LinearRegression()\n",
    "model.fit(df_2016['population'].values.reshape(-1, 1), df_2016['gdp'].values.reshape(-1, 1))\n",
    "\n",
    "# plot the data along with predictions from the linear regression model\n",
    "inputs = np.linspace(1, 2000000000, num=50)\n",
    "predictions = model.predict(inputs.reshape(-1,1))\n",
    "\n",
    "df_2016.plot('population', 'gdp', kind='scatter')\n",
    "plt.plot(inputs, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the United States to see what happens with the linear regression model\n",
    "df_2016[df_2016['Country Name'] != 'United States'].plot('population', 'gdp', kind='scatter')\n",
    "# plt.plot(inputs, predictions)\n",
    "model.fit(df_2016[df_2016['Country Name'] != 'United States']['population'].values.reshape(-1, 1), \n",
    "          df_2016[df_2016['Country Name'] != 'United States']['gdp'].values.reshape(-1, 1))\n",
    "inputs = np.linspace(1, 2000000000, num=50)\n",
    "predictions = model.predict(inputs.reshape(-1,1))\n",
    "plt.plot(inputs, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the code now ouputs a GDP value of 5.26e+12 when population equals 1e9. In other words, removing the United States shifted the linear regression line down.\n",
    "\n",
    "Data scientists sometimes have the task of creating an outlier removal model. In this exercise, you've used the Tukey rule. There are other one-dimensional models like eliminating data that is far from the mean. There are also more sophisticated models that take into account multi-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Eliminating Outliers\n",
    "\n",
    "Eliminating outliers is a big topic. There are many different ways to eliminate outliers. A data engineer's job isn't necessarily to decide what counts as an outlier and what does not. A data scientist would determine that. The data engineer would code the algorithms that eliminate outliers from a data set based on any criteria that a data scientist has decided.\n",
    "\n",
    "In this exercise, you'll write code to eliminate outliers based on the Tukey rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# read in the projects data set and do basic wrangling \n",
    "gdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\n",
    "gdp.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "population = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\n",
    "population.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Reshape the data sets so that they are in long format\n",
    "gdp_melt = gdp.melt(id_vars=['Country Name'], \n",
    "                    var_name='year', \n",
    "                    value_name='gdp')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing gdp values\n",
    "gdp_melt['gdp'] = gdp_melt.sort_values('year').groupby('Country Name')['gdp'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "population_melt = population.melt(id_vars=['Country Name'], \n",
    "                                  var_name='year', \n",
    "                                  value_name='population')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing population values\n",
    "population_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# merge the population and gdp data together into one data frame\n",
    "df_country = gdp_melt.merge(population_melt, on=('Country Name', 'year'))\n",
    "\n",
    "# filter data for the year 2016\n",
    "df_2016 = df_country[df_country['year'] == '2016']\n",
    "\n",
    "# filter out values that are not countries\n",
    "non_countries = ['World',\n",
    " 'High income',\n",
    " 'OECD members',\n",
    " 'Post-demographic dividend',\n",
    " 'IDA & IBRD total',\n",
    " 'Low & middle income',\n",
    " 'Middle income',\n",
    " 'IBRD only',\n",
    " 'East Asia & Pacific',\n",
    " 'Europe & Central Asia',\n",
    " 'North America',\n",
    " 'Upper middle income',\n",
    " 'Late-demographic dividend',\n",
    " 'European Union',\n",
    " 'East Asia & Pacific (excluding high income)',\n",
    " 'East Asia & Pacific (IDA & IBRD countries)',\n",
    " 'Euro area',\n",
    " 'Early-demographic dividend',\n",
    " 'Lower middle income',\n",
    " 'Latin America & Caribbean',\n",
    " 'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    " 'Latin America & Caribbean (excluding high income)',\n",
    " 'Europe & Central Asia (IDA & IBRD countries)',\n",
    " 'Middle East & North Africa',\n",
    " 'Europe & Central Asia (excluding high income)',\n",
    " 'South Asia (IDA & IBRD)',\n",
    " 'South Asia',\n",
    " 'Arab World',\n",
    " 'IDA total',\n",
    " 'Sub-Saharan Africa',\n",
    " 'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    " 'Sub-Saharan Africa (excluding high income)',\n",
    " 'Middle East & North Africa (excluding high income)',\n",
    " 'Middle East & North Africa (IDA & IBRD countries)',\n",
    " 'Central Europe and the Baltics',\n",
    " 'Pre-demographic dividend',\n",
    " 'IDA only',\n",
    " 'Least developed countries: UN classification',\n",
    " 'IDA blend',\n",
    " 'Fragile and conflict affected situations',\n",
    " 'Heavily indebted poor countries (HIPC)',\n",
    " 'Low income',\n",
    " 'Small states',\n",
    " 'Other small states',\n",
    " 'Not classified',\n",
    " 'Caribbean small states',\n",
    " 'Pacific island small states']\n",
    "\n",
    "# remove non countries from the data\n",
    "df_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]\n",
    "\n",
    "\n",
    "# plot the data\n",
    "x = list(df_2016['population'])\n",
    "y = list(df_2016['gdp'])\n",
    "text = df_2016['Country Name']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(x, y)\n",
    "plt.title('GDP vs Population')\n",
    "plt.xlabel('GDP')\n",
    "plt.ylabel('Population')\n",
    "for i, txt in enumerate(text):\n",
    "    ax.annotate(txt, (x[i],y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that uses the Tukey rule to eliminate outliers from an array of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that uses the Tukey rule to detect outliers in a dataframe column \n",
    "# and then removes that entire row from the data frame. For example, if the United States \n",
    "# is detected to be a GDP outlier, then remove the entire row of United States data.\n",
    "# The function inputs should be a data frame and a column name.\n",
    "# The output is a data_frame with the outliers eliminated\n",
    "\n",
    "# HINT: Re-use code from the previous exercise\n",
    "\n",
    "def tukey_rule(data_frame, column_name):\n",
    "    data = data_frame[column_name]\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    max_value = Q3 + 1.5 * IQR\n",
    "    min_value = Q1 - 1.5 * IQR\n",
    "    \n",
    "    return data_frame[(data_frame[column_name] < max_value) & (data_frame[column_name] > min_value)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the function to eliminate population outliers and then gdp outliers from the dataframe. Store results in the df_outlier_removed variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tukey_rule() function to make a new data frame with gdp and population outliers removed\n",
    "# Put the results in the df_outlier_removed variable\n",
    "\n",
    "df_outlier_removed = df_2016.copy()\n",
    "\n",
    "for column in ['population','gdp']:\n",
    "    df_outlier_removed = tukey_rule(df_outlier_removed, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "x = list(df_outlier_removed['population'])\n",
    "y = list(df_outlier_removed['gdp'])\n",
    "text = df_outlier_removed['Country Name']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(x, y)\n",
    "plt.title('GDP vs Population')\n",
    "plt.xlabel('GDP')\n",
    "plt.ylabel('Population')\n",
    "for i, txt in enumerate(text):\n",
    "    ax.annotate(txt, (x[i],y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Scaling Data\n",
    "\n",
    "In this exercise, you'll practice scaling data. Sometimes, you'll see the terms **standardization** and **normalization** used interchangeably when referring to feature scaling. However, these are slightly different operations. Standardization refers to scaling a set of values so that they have a mean of zero and a standard deviation of one. Normalization refers to scaling a set of values so that the range if between zero and one.\n",
    "\n",
    "In this exercise, you'll practice implementing standardization and normalization in code. There are libraries, like scikit-learn, that can do this for you; however, in data engineering, you might not always have these tools available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# read in the projects data set and do basic wrangling \n",
    "gdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\n",
    "gdp.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "population = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\n",
    "population.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Reshape the data sets so that they are in long format\n",
    "gdp_melt = gdp.melt(id_vars=['Country Name'], \n",
    "                    var_name='year', \n",
    "                    value_name='gdp')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing gdp values\n",
    "gdp_melt['gdp'] = gdp_melt.sort_values('year').groupby('Country Name')['gdp'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "population_melt = population.melt(id_vars=['Country Name'], \n",
    "                                  var_name='year', \n",
    "                                  value_name='population')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing population values\n",
    "population_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# merge the population and gdp data together into one data frame\n",
    "df_country = gdp_melt.merge(population_melt, on=('Country Name', 'year'))\n",
    "\n",
    "# filter data for the year 2016\n",
    "df_2016 = df_country[df_country['year'] == '2016']\n",
    "\n",
    "# filter out values that are not countries\n",
    "non_countries = ['World',\n",
    " 'High income',\n",
    " 'OECD members',\n",
    " 'Post-demographic dividend',\n",
    " 'IDA & IBRD total',\n",
    " 'Low & middle income',\n",
    " 'Middle income',\n",
    " 'IBRD only',\n",
    " 'East Asia & Pacific',\n",
    " 'Europe & Central Asia',\n",
    " 'North America',\n",
    " 'Upper middle income',\n",
    " 'Late-demographic dividend',\n",
    " 'European Union',\n",
    " 'East Asia & Pacific (excluding high income)',\n",
    " 'East Asia & Pacific (IDA & IBRD countries)',\n",
    " 'Euro area',\n",
    " 'Early-demographic dividend',\n",
    " 'Lower middle income',\n",
    " 'Latin America & Caribbean',\n",
    " 'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    " 'Latin America & Caribbean (excluding high income)',\n",
    " 'Europe & Central Asia (IDA & IBRD countries)',\n",
    " 'Middle East & North Africa',\n",
    " 'Europe & Central Asia (excluding high income)',\n",
    " 'South Asia (IDA & IBRD)',\n",
    " 'South Asia',\n",
    " 'Arab World',\n",
    " 'IDA total',\n",
    " 'Sub-Saharan Africa',\n",
    " 'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    " 'Sub-Saharan Africa (excluding high income)',\n",
    " 'Middle East & North Africa (excluding high income)',\n",
    " 'Middle East & North Africa (IDA & IBRD countries)',\n",
    " 'Central Europe and the Baltics',\n",
    " 'Pre-demographic dividend',\n",
    " 'IDA only',\n",
    " 'Least developed countries: UN classification',\n",
    " 'IDA blend',\n",
    " 'Fragile and conflict affected situations',\n",
    " 'Heavily indebted poor countries (HIPC)',\n",
    " 'Low income',\n",
    " 'Small states',\n",
    " 'Other small states',\n",
    " 'Not classified',\n",
    " 'Caribbean small states',\n",
    " 'Pacific island small states']\n",
    "\n",
    "# remove non countries from the data\n",
    "df_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]\n",
    "\n",
    "\n",
    "# show the first ten rows\n",
    "print('first ten rows of data')\n",
    "df_2016.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the Data\n",
    "\n",
    "To normalize data, you take a feature, like gdp, and use the following formula\n",
    "\n",
    "$x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "where \n",
    "* x is a value of gdp\n",
    "* x_max is the maximum gdp in the data\n",
    "* x_min is the minimum GDP in the data\n",
    "\n",
    "First, write a function that outputs the x_min and x_max values of an array. The inputs are an array of data (like the GDP data). The outputs are the x_min and x_max values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_min_max(data):\n",
    "    minimum = min(data)\n",
    "    maximum = max(data)\n",
    "    return minimum, maximum\n",
    "\n",
    "x_min_max(df_2016['gdp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function that normalizes a data point. The inputs are an x value, a minimum value, and a maximum value. The output is the normalized data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, x_min, x_max):\n",
    "    # Complete this function\n",
    "    # The input is a single value \n",
    "    # The output is the normalized value\n",
    "    return (x - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are you making these separate functions? Let's say you are training a machine learning model and using normalized GDP as a feature. As new data comes in, you'll want to make predictions using the new GDP data. You'll have to normalize this incoming data. To do that, you need to store the x_min and x_max from the training set. Hence the x_min_max() function gives you the minimum and maximum values, which you can then store in a variable.\n",
    "\n",
    "\n",
    "A good way to keep track of the minimum and maximum values would be to use a class. In this next section, fill out the Normalizer() class code to make a class that normalizes a data set and stores min and max values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    # Complete the normalizer class\n",
    "    # The normalizer class receives a dataframe as its only input for initialization\n",
    "    # For example, the data frame might contain gdp and population data in two separate columns\n",
    "    # Follow the TODOs in each section\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "        \n",
    "        # complete the init function. \n",
    "        # Assume the dataframe has an unknown number of columns like [['gdp', 'population']] \n",
    "        # iterate through each column calculating the min and max for each column\n",
    "        # append the results to the params attribute list\n",
    "        \n",
    "        # For example, take the gdp column and calculate the minimum and maximum\n",
    "        # Put these results in a list [minimum, maximum]\n",
    "        # Append the list to the params variable\n",
    "        # Then take the population column and do the same\n",
    "        \n",
    "        # HINT: You can put your x_min_max() function as part of this class and use it\n",
    "        \n",
    "        self.params = []\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            self.params.append(x_min_max(dataframe[column]))\n",
    "            \n",
    "    def x_min_max(data):\n",
    "        # complete the x_min_max method\n",
    "        # HINT: You can use the same function defined earlier in the exercise\n",
    "        minimum = min(data)\n",
    "        maximum = max(data)\n",
    "        return minimum, maximum\n",
    "\n",
    "    def normalize_data(self, x):\n",
    "        # complete the normalize_data method\n",
    "        # The function receives a data point as an input and then outputs the normalized version\n",
    "        # For example, if an input data point of [gdp, population] were used. Then the output would\n",
    "        # be the normalized version of the [gdp, population] data point\n",
    "        # Put the results in the normalized variable defined below\n",
    "        \n",
    "        # Assume that the columns in the dataframe used to initialize an object are in the same\n",
    "        # order as this data point x\n",
    "        \n",
    "        # HINT: You cannot use the normalize_data function defined earlier in the exercise.\n",
    "        # You'll need to iterate through the individual values in the x variable        \n",
    "        # Use the params attribute where the min and max values are stored \n",
    "        normalized = []\n",
    "        for i, value in enumerate(x):\n",
    "            x_max = self.params[i][1]\n",
    "            x_min = self.params[i][0]\n",
    "            normalized.append((x[i] - x_min) / (x_max - x_min))\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_normalizer = Normalizer(df_2016[['gdp', 'population']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_normalizer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_normalizer.normalize_data([13424475000000.0, 1300000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When normalizing or standardizing features for machine learning, you'll need to store the parameters you used to do the scaling. That way you can scale new data points when making predictions. In this exercise, you stored the minimum and maximum values of a feature. When standardizing data, you would need to store the mean and standard deviation. The standardization formula is:\n",
    "\n",
    "$x_{standardized} = \\frac{x - \\overline{x}}{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Feature Engineering\n",
    "\n",
    "Practice creating new features from the GDP and population data. \n",
    "\n",
    "You'll create a new feature gdppercapita, which is GDP divided by population. You'll then write code to create new features like GDP squared and GDP cubed. \n",
    "\n",
    "Start by running the code below. It reads in the World Bank data, filters the data for the year 2016, and cleans the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# read in the projects data set and do basic wrangling \n",
    "gdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\n",
    "gdp.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "population = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\n",
    "population.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Reshape the data sets so that they are in long format\n",
    "gdp_melt = gdp.melt(id_vars=['Country Name'], \n",
    "                    var_name='year', \n",
    "                    value_name='gdp')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing gdp values\n",
    "gdp_melt['gdp'] = gdp_melt.sort_values('year').groupby('Country Name')['gdp'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "population_melt = population.melt(id_vars=['Country Name'], \n",
    "                                  var_name='year', \n",
    "                                  value_name='population')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing population values\n",
    "population_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# merge the population and gdp data together into one data frame\n",
    "df_country = gdp_melt.merge(population_melt, on=('Country Name', 'year'))\n",
    "\n",
    "# filter data for the year 2016\n",
    "df_2016 = df_country[df_country['year'] == '2016']\n",
    "\n",
    "# filter out values that are not countries\n",
    "non_countries = ['World',\n",
    " 'High income',\n",
    " 'OECD members',\n",
    " 'Post-demographic dividend',\n",
    " 'IDA & IBRD total',\n",
    " 'Low & middle income',\n",
    " 'Middle income',\n",
    " 'IBRD only',\n",
    " 'East Asia & Pacific',\n",
    " 'Europe & Central Asia',\n",
    " 'North America',\n",
    " 'Upper middle income',\n",
    " 'Late-demographic dividend',\n",
    " 'European Union',\n",
    " 'East Asia & Pacific (excluding high income)',\n",
    " 'East Asia & Pacific (IDA & IBRD countries)',\n",
    " 'Euro area',\n",
    " 'Early-demographic dividend',\n",
    " 'Lower middle income',\n",
    " 'Latin America & Caribbean',\n",
    " 'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    " 'Latin America & Caribbean (excluding high income)',\n",
    " 'Europe & Central Asia (IDA & IBRD countries)',\n",
    " 'Middle East & North Africa',\n",
    " 'Europe & Central Asia (excluding high income)',\n",
    " 'South Asia (IDA & IBRD)',\n",
    " 'South Asia',\n",
    " 'Arab World',\n",
    " 'IDA total',\n",
    " 'Sub-Saharan Africa',\n",
    " 'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    " 'Sub-Saharan Africa (excluding high income)',\n",
    " 'Middle East & North Africa (excluding high income)',\n",
    " 'Middle East & North Africa (IDA & IBRD countries)',\n",
    " 'Central Europe and the Baltics',\n",
    " 'Pre-demographic dividend',\n",
    " 'IDA only',\n",
    " 'Least developed countries: UN classification',\n",
    " 'IDA blend',\n",
    " 'Fragile and conflict affected situations',\n",
    " 'Heavily indebted poor countries (HIPC)',\n",
    " 'Low income',\n",
    " 'Small states',\n",
    " 'Other small states',\n",
    " 'Not classified',\n",
    " 'Caribbean small states',\n",
    " 'Pacific island small states']\n",
    "\n",
    "# remove non countries from the data\n",
    "df_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]\n",
    "df_2016.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new feature called gdppercapita in a new column. This feature should be the gdp value divided by the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature called gdppercapita, \n",
    "#      which is the gdp value divided by the population value for each country\n",
    "\n",
    "df_2016['gdppercapita'] = df_2016['gdp'] / df_2016['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load \n",
    "\n",
    "In this part, I'll load data into different formats: a csv file, a json file, and a SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# read in the projects data set and do basic wrangling \n",
    "gdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\n",
    "gdp.drop(['Unnamed: 62', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "population = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\n",
    "population.drop(['Unnamed: 62', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Reshape the data sets so that they are in long format\n",
    "gdp_melt = gdp.melt(id_vars=['Country Name', 'Country Code'], \n",
    "                    var_name='year', \n",
    "                    value_name='gdp')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing gdp values\n",
    "gdp_melt['gdp'] = gdp_melt.sort_values('year').groupby(['Country Name', 'Country Code'])['gdp'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "population_melt = population.melt(id_vars=['Country Name', 'Country Code'], \n",
    "                                  var_name='year', \n",
    "                                  value_name='population')\n",
    "\n",
    "# Use back fill and forward fill to fill in missing population values\n",
    "population_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# merge the population and gdp data together into one data frame\n",
    "df_indicator = gdp_melt.merge(population_melt, on=('Country Name', 'Country Code', 'year'))\n",
    "\n",
    "# filter out values that are not countries\n",
    "non_countries = ['World',\n",
    " 'High income',\n",
    " 'OECD members',\n",
    " 'Post-demographic dividend',\n",
    " 'IDA & IBRD total',\n",
    " 'Low & middle income',\n",
    " 'Middle income',\n",
    " 'IBRD only',\n",
    " 'East Asia & Pacific',\n",
    " 'Europe & Central Asia',\n",
    " 'North America',\n",
    " 'Upper middle income',\n",
    " 'Late-demographic dividend',\n",
    " 'European Union',\n",
    " 'East Asia & Pacific (excluding high income)',\n",
    " 'East Asia & Pacific (IDA & IBRD countries)',\n",
    " 'Euro area',\n",
    " 'Early-demographic dividend',\n",
    " 'Lower middle income',\n",
    " 'Latin America & Caribbean',\n",
    " 'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    " 'Latin America & Caribbean (excluding high income)',\n",
    " 'Europe & Central Asia (IDA & IBRD countries)',\n",
    " 'Middle East & North Africa',\n",
    " 'Europe & Central Asia (excluding high income)',\n",
    " 'South Asia (IDA & IBRD)',\n",
    " 'South Asia',\n",
    " 'Arab World',\n",
    " 'IDA total',\n",
    " 'Sub-Saharan Africa',\n",
    " 'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    " 'Sub-Saharan Africa (excluding high income)',\n",
    " 'Middle East & North Africa (excluding high income)',\n",
    " 'Middle East & North Africa (IDA & IBRD countries)',\n",
    " 'Central Europe and the Baltics',\n",
    " 'Pre-demographic dividend',\n",
    " 'IDA only',\n",
    " 'Least developed countries: UN classification',\n",
    " 'IDA blend',\n",
    " 'Fragile and conflict affected situations',\n",
    " 'Heavily indebted poor countries (HIPC)',\n",
    " 'Low income',\n",
    " 'Small states',\n",
    " 'Other small states',\n",
    " 'Not classified',\n",
    " 'Caribbean small states',\n",
    " 'Pacific island small states']\n",
    "\n",
    "# remove non countries from the data\n",
    "df_indicator  = df_indicator[~df_indicator['Country Name'].isin(non_countries)]\n",
    "df_indicator.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_indicator.columns = ['countryname', 'countrycode', 'year', 'gdp', 'population']\n",
    "\n",
    "# output the first few rows of the data frame\n",
    "df_indicator.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code cell to read in the countries data set. This will create a data frame called df_projects containing the World Bank projects data. The data frame only has the 'id', 'countryname', 'countrycode', 'totalamt', and 'year' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycountry\n",
    "from pycountry import countries\n",
    "\n",
    "# read in the projects data set with all columns type string\n",
    "df_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\n",
    "df_projects.drop(['Unnamed: 56'], axis=1, inplace=True)\n",
    "\n",
    "df_projects['countryname'] = df_projects['countryname'].str.split(';').str.get(0)\n",
    "\n",
    "# set up the libraries and variables\n",
    "from collections import defaultdict\n",
    "country_not_found = [] # stores countries not found in the pycountry library\n",
    "project_country_abbrev_dict = defaultdict(str) # set up an empty dictionary of string values\n",
    "\n",
    "# iterate through the country names in df_projects. \n",
    "# Create a dictionary mapping the country name to the alpha_3 ISO code\n",
    "for country in df_projects['countryname'].drop_duplicates().sort_values():\n",
    "    try: \n",
    "        # look up the country name in the pycountry library\n",
    "        # store the country name as the dictionary key and the ISO-3 code as the value\n",
    "        project_country_abbrev_dict[country] = countries.lookup(country).alpha_3\n",
    "    except:\n",
    "        # If the country name is not in the pycountry library, then print out the country name\n",
    "        # And store the results in the country_not_found list\n",
    "        country_not_found.append(country)\n",
    "        \n",
    "# run this code cell to load the dictionary\n",
    "\n",
    "country_not_found_mapping = {'Co-operative Republic of Guyana': 'GUY',\n",
    "             'Commonwealth of Australia':'AUS',\n",
    "             'Democratic Republic of Sao Tome and Prin':'STP',\n",
    "             'Democratic Republic of the Congo':'COD',\n",
    "             'Democratic Socialist Republic of Sri Lan':'LKA',\n",
    "             'East Asia and Pacific':'EAS',\n",
    "             'Europe and Central Asia': 'ECS',\n",
    "             'Islamic  Republic of Afghanistan':'AFG',\n",
    "             'Latin America':'LCN',\n",
    "              'Caribbean':'LCN',\n",
    "             'Macedonia':'MKD',\n",
    "             'Middle East and North Africa':'MEA',\n",
    "             'Oriental Republic of Uruguay':'URY',\n",
    "             'Republic of Congo':'COG',\n",
    "             \"Republic of Cote d'Ivoire\":'CIV',\n",
    "             'Republic of Korea':'KOR',\n",
    "             'Republic of Niger':'NER',\n",
    "             'Republic of Kosovo':'XKX',\n",
    "             'Republic of Rwanda':'RWA',\n",
    "              'Republic of The Gambia':'GMB',\n",
    "              'Republic of Togo':'TGO',\n",
    "              'Republic of the Union of Myanmar':'MMR',\n",
    "              'Republica Bolivariana de Venezuela':'VEN',\n",
    "              'Sint Maarten':'SXM',\n",
    "              \"Socialist People's Libyan Arab Jamahiriy\":'LBY',\n",
    "              'Socialist Republic of Vietnam':'VNM',\n",
    "              'Somali Democratic Republic':'SOM',\n",
    "              'South Asia':'SAS',\n",
    "              'St. Kitts and Nevis':'KNA',\n",
    "              'St. Lucia':'LCA',\n",
    "              'St. Vincent and the Grenadines':'VCT',\n",
    "              'State of Eritrea':'ERI',\n",
    "              'The Independent State of Papua New Guine':'PNG',\n",
    "              'West Bank and Gaza':'PSE',\n",
    "              'World':'WLD'}\n",
    "\n",
    "project_country_abbrev_dict.update(country_not_found_mapping)\n",
    "\n",
    "df_projects['countrycode'] = df_projects['countryname'].apply(lambda x: project_country_abbrev_dict[x])\n",
    "\n",
    "df_projects['boardapprovaldate'] = pd.to_datetime(df_projects['boardapprovaldate'])\n",
    "\n",
    "df_projects['year'] = df_projects['boardapprovaldate'].dt.year.astype(str).str.slice(stop=4)\n",
    "\n",
    "df_projects['totalamt'] = pd.to_numeric(df_projects['totalamt'].str.replace(',',\"\"))\n",
    "\n",
    "df_projects = df_projects[['id', 'countryname', 'countrycode', 'totalamt', 'year']]\n",
    "\n",
    "df_projects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few cells in this workbook loaded and cleaned the World Bank Data. You now have two data frames:\n",
    "* df_projects, which contain data from the projects data set\n",
    "* df_indicator, which contain population and gdp data for various years\n",
    "\n",
    "They both have country code variables. Note, however, that there could be countries represented in the projects data set that are not in the indicator data set and vice versus.\n",
    "\n",
    "In this first exercise, merge the two data sets together using country code and year as common keys. When joining the data sets, keep all of the data in the df_projects dataframe even if there is no indicator data for that country code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_projects.merge(df_indicator, how='left', on=['countrycode', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[(df_merged['year'] == '2017') & (df_merged['countryname_y'] == 'Jordan')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the df_merged dataframe as a json file. You can use the pandas [to_json() method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_json('countrydata.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the df_merged dataframe as a csv file. You can use the pandas [to_csv() method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('countrydata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the df_merged dataframe as a sqlite database file. For this exercise, you can put all of the data as one table. In the next exercise, you'll create a database with multiple tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# connect to the database\n",
    "# the database file will be worldbank.db\n",
    "# note that sqlite3 will create this database file if it does not exist already\n",
    "conn = sqlite3.connect('worldbank.db')\n",
    "\n",
    "df_merged.to_sql('merged', con = conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT * FROM merged WHERE year = \"2017\" AND countrycode = \"BRA\"', con = conn).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the data to a SQL database like in the previous exercise; however, this time, put the df_indicator data in one table and the df_projects data in another table. Call the df_indicator table 'indicator' and the df_projects table 'projects'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# connect to the database\n",
    "# the database file will be worldbank.db\n",
    "# note that sqlite3 will create this database file if it does not exist already\n",
    "conn = sqlite3.connect('worldbank.db')\n",
    "\n",
    "df_indicator.to_sql('indicator', con = conn, if_exists='replace', index=False)\n",
    "df_projects.to_sql('projects', con = conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT * FROM projects LEFT JOIN indicator ON \\\n",
    "projects.countrycode = indicator.countrycode AND \\\n",
    "projects.year = indicator.year WHERE \\\n",
    "projects.year = \"2017\" AND projects.countrycode = \"BRA\"', con = conn).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commit any changes to the database and close the database\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
